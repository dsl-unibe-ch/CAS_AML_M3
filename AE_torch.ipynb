{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smjlo1x48N3t"
      },
      "source": [
        "# Autoencoders: Learning Compact Representations\n",
        "\n",
        "<div>\n",
        "<h2 style=\"margin: 0;\">Learning Objectives</h2>\n",
        "<ul style=\"margin: 10px 0;\">\n",
        "<li>Understand the architecture and working principles of autoencoders</li>\n",
        "<li>Implement and train simple and convolutional autoencoders</li>\n",
        "<li>Explore latent space representations and their properties</li>\n",
        "<li>Apply autoencoders for denoising and outlier detection</li>\n",
        "<li>Visualize and interpret learned representations</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup & Data Loading\n",
        "2. Simple Autoencoder\n",
        "3. Denoising Autoencoder\n",
        "4. Convolutional Autoencoder\n",
        "5. Latent Space Exploration\n",
        "6. Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6H2W9QmdP0U"
      },
      "source": [
        "<div>\n",
        "<p style=\"margin: 0;\">\n",
        "<strong>CAS on Advanced Machine Learning</strong><br>\n",
        "Data Science Lab, University of Bern, 2025<br>\n",
        "Prepared by Dr. Mykhailo Vladymyrov and Matteo Boi\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "\n",
        "<strong>Autoencoder architecture</strong><br>\n",
        "\n",
        "<img src=\"https://contenthub-static.grammarly.com/blog/wp-content/uploads/2024/10/6303_blog-visuals-auto-encoders_1500X800.png\" alt=\"Autoencoder Concept\" style=\"width: 800px; height: auto; border-radius: 10px;\">\n",
        "\n",
        "Figure from <a href=\"https://www.grammarly.com/blog/ai/what-is-autoencoder/\">Grammarly</a>.\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "This work is licensed under a <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncD-sYgs0hs_"
      },
      "source": [
        "# Libraries and Utilities\n",
        "\n",
        "<div>\n",
        "<strong>Note:</strong> We'll be using PyTorch for implementing autoencoders, along with visualization libraries for better understanding of the learned representations.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GynY3qcttQPu"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install mlflow\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional packages for interactive widgets\n",
        "!pip install ipywidgets\n",
        "!pip install plotly\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjo8ffOptAcP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# set env var to allow duplicated lib\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRCK8pzztAcQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import einops as eo\n",
        "import pathlib as pl\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import collections  as mc\n",
        "from matplotlib import animation\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import entropy\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from time import time as timer\n",
        "#import umap\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Audio\n",
        "import IPython\n",
        "\n",
        "import tqdm.auto as tqdm\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sys\n",
        "is_colab = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting Configuration\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (10, 6),\n",
        "    'axes.titlesize': 16,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.titlesize': 18,\n",
        "    'lines.linewidth': 2,\n",
        "    'axes.grid': True,\n",
        "    'grid.alpha': 0.3\n",
        "})\n",
        "\n",
        "COLORS = {\n",
        "    'encoder': '#FF6B6B',\n",
        "    'decoder': '#4ECDC4', \n",
        "    'latent': '#45B7D1',\n",
        "    'original': '#96CEB4',\n",
        "    'reconstructed': '#FECA57',\n",
        "    'noisy': '#FF9FF3'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Widgets Setup\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def create_interactive_latent_explorer(model, samples, device):\n",
        "    \"\"\"\n",
        "    Create interactive widget to explore latent space\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract data\n",
        "    sample_images = torch.tensor(samples['images']).to(device)\n",
        "    labels = samples['labels']\n",
        "    \n",
        "    # Get latent representations\n",
        "    with torch.no_grad():\n",
        "        latent_codes = model.encode(sample_images)\n",
        "        latent_np = latent_codes.cpu().numpy()\n",
        "    \n",
        "    # Create sliders for each latent dimension\n",
        "    if latent_codes.shape[1] >= 2:\n",
        "        slider1 = widgets.FloatSlider(value=0, min=-6, max=6, step=0.1, description='Latent 1:')\n",
        "        slider2 = widgets.FloatSlider(value=0, min=-6, max=6, step=0.1, description='Latent 2:')\n",
        "        \n",
        "        output = widgets.Output()\n",
        "        \n",
        "        def update_plot(change):\n",
        "            with output:\n",
        "                clear_output(wait=True)\n",
        "                \n",
        "                # Create latent vector\n",
        "                z = torch.zeros(1, latent_codes.shape[1]).to(device)\n",
        "                z[0, 0] = slider1.value\n",
        "                z[0, 1] = slider2.value\n",
        "                \n",
        "                # Generate image\n",
        "                with torch.no_grad():\n",
        "                    generated = model.decode(z)\n",
        "                    img = to_np_showable(generated[0])\n",
        "                \n",
        "                # Plot\n",
        "                fig, ax = plt.subplots(figsize=(6, 6))\n",
        "                ax.imshow(img, cmap='gray')\n",
        "                ax.set_title(f'Generated Image\\\\nLatent: [{slider1.value:.1f}, {slider2.value:.1f}]', \n",
        "                           fontsize=14)\n",
        "                ax.axis('off')\n",
        "                plt.show()\n",
        "        \n",
        "        slider1.observe(update_plot, names='value')\n",
        "        slider2.observe(update_plot, names='value')\n",
        "        \n",
        "        # Initial plot\n",
        "        update_plot(None)\n",
        "        \n",
        "        return widgets.VBox([\n",
        "            widgets.HTML(\"<h3> Interactive Latent Space Explorer</h3>\"),\n",
        "            widgets.HTML(\"<p>Move the sliders to explore what the decoder generates!</p>\"),\n",
        "            slider1, slider2, output\n",
        "        ])\n",
        "    \n",
        "    return widgets.HTML(\"<p>Interactive explorer requires at least 2 latent dimensions</p>\")\n",
        "\n",
        "def create_noise_comparison_widget():\n",
        "    \"\"\"\n",
        "    Create widget to interactively compare different noise levels\n",
        "    \"\"\"\n",
        "    \n",
        "    noise_slider = widgets.FloatSlider(\n",
        "        value=0.2, min=0.0, max=0.8, step=0.1,\n",
        "        description='Noise Rate:', style={'description_width': 'initial'}\n",
        "    )\n",
        "    \n",
        "    output = widgets.Output()\n",
        "    \n",
        "    def update_noise_demo(change):\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            # Create demo with current noise rate\n",
        "            noise_rate = noise_slider.value\n",
        "            \n",
        "            # Simulate noisy image (using a sample from your data)\n",
        "            sample_idx = 42  # Fixed sample for consistency\n",
        "            original_img = samples['images'][sample_idx, 0]  # Assuming samples is available\n",
        "            \n",
        "            # Add noise\n",
        "            noise_mask = np.random.binomial(1, noise_rate, original_img.shape)\n",
        "            sp_noise = np.random.binomial(1, 0.5, original_img.shape) - 0.5\n",
        "            noisy_img = original_img * (1 - noise_mask) + sp_noise * noise_mask\n",
        "            \n",
        "            # Plot comparison\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "            \n",
        "            axes[0].imshow(original_img, cmap='gray')\n",
        "            axes[0].set_title('Original Image', fontsize=14)\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            axes[1].imshow(noisy_img, cmap='gray')\n",
        "            axes[1].set_title(f'Noisy Image (rate={noise_rate:.1f})', fontsize=14)\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            plt.suptitle('Effect of Different Noise Levels', fontsize=16)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    \n",
        "    noise_slider.observe(update_noise_demo, names='value')\n",
        "    update_noise_demo(None)  # Initial plot\n",
        "    \n",
        "    return widgets.VBox([\n",
        "        widgets.HTML(\"<h3> Interactive Noise Level Demo</h3>\"),\n",
        "        noise_slider, output\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlSvdkTItAcR"
      },
      "outputs": [],
      "source": [
        "# get mean and std of an array with numpy:\n",
        "def get_mean_std(x):\n",
        "    x_mean = np.mean(x)\n",
        "    x_std = np.std(x)\n",
        "    return x_mean, x_std\n",
        "\n",
        "# get min and max of an array with numpy:\n",
        "def get_min_max(x):\n",
        "    x_min = np.min(x)\n",
        "    x_max = np.max(x)\n",
        "    return x_min, x_max\n",
        "\n",
        "def is_iterable(obj):\n",
        "    try:\n",
        "        iter(obj)\n",
        "    except Exception:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def type_len(obj):\n",
        "    t = type(obj)\n",
        "    if is_iterable(obj):\n",
        "        sfx = f', shape: {obj.shape}' if t == np.ndarray else ''\n",
        "        print(f'type: {t}, len: {len(obj)}{sfx}')\n",
        "    else:\n",
        "        print(f'type: {t}, len: {len(obj)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N78bzRywtAcc"
      },
      "outputs": [],
      "source": [
        "# Image Visualization Functions\n",
        "\n",
        "def to_np_showable(pt_img):\n",
        "    \"\"\"Convert PyTorch tensor to displayable numpy array\"\"\"\n",
        "    np_im = pt_img.detach().cpu().numpy()\n",
        "    if len(np_im.shape) == 4:\n",
        "        np_im = np_im[0]\n",
        "    if np_im.shape[0] > 3:\n",
        "        np_im = np_im[-3:]\n",
        "    return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
        "\n",
        "def plot_im(im, title=\"Image\", is_torch=True, figsize=(6, 6)):\n",
        "    \"\"\"Plot a single image with enhanced styling\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(to_np_showable(im) if is_torch else im, cmap='gray')\n",
        "    ax.set_title(title, fontsize=14, pad=15)\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_im_samples(ds, n=5, is_torch=False, title=\"Sample Images\", figsize=None):\n",
        "    \"\"\"Plot multiple images in a row with enhanced styling\"\"\"\n",
        "    if figsize is None:\n",
        "        figsize = (3*n, 4)\n",
        "    \n",
        "    fig, axs = plt.subplots(1, n, figsize=figsize)\n",
        "    fig.suptitle(title, fontsize=16, y=1.02)\n",
        "    \n",
        "    if n == 1:\n",
        "        axs = [axs]\n",
        "    \n",
        "    for i, image in enumerate(ds[:n]):\n",
        "        axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
        "        axs[i].set_title(f'Sample {i+1}', fontsize=12)\n",
        "        axs[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_comparison(original, reconstructed, noisy=None, titles=None, figsize=(15, 5)):\n",
        "    \"\"\"Plot comparison between original, noisy (if provided), and reconstructed images\"\"\"\n",
        "    images = [original, reconstructed]\n",
        "    default_titles = ['Original', 'Reconstructed']\n",
        "    \n",
        "    if noisy is not None:\n",
        "        images.insert(1, noisy)\n",
        "        default_titles = ['Original', 'Noisy Input', 'Reconstructed']\n",
        "    \n",
        "    if titles is None:\n",
        "        titles = default_titles\n",
        "    \n",
        "    n_imgs = len(images)\n",
        "    fig, axs = plt.subplots(1, n_imgs, figsize=figsize)\n",
        "    \n",
        "    colors = [COLORS['original'], COLORS['noisy'], COLORS['reconstructed']][:n_imgs]\n",
        "    \n",
        "    for i, (img, title, color) in enumerate(zip(images, titles, colors)):\n",
        "        axs[i].imshow(to_np_showable(img), cmap='gray')\n",
        "        axs[i].set_title(title, fontsize=14, color=color, fontweight='bold')\n",
        "        axs[i].axis('off')\n",
        "        \n",
        "        # Add colored border\n",
        "        for spine in axs[i].spines.values():\n",
        "            spine.set_edgecolor(color)\n",
        "            spine.set_linewidth(3)\n",
        "            spine.set_visible(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyC2UKsetAce"
      },
      "outputs": [],
      "source": [
        "def mosaic(mtr_of_ims, spacing=2, background_color=0.8):\n",
        "    \"\"\"\n",
        "    Create a mosaic from a 2D matrix of images with enhanced styling\n",
        "    \n",
        "    Args:\n",
        "        mtr_of_ims: 2D list/array of images\n",
        "        spacing: pixels between images\n",
        "        background_color: color for spacing between images\n",
        "    \"\"\"\n",
        "    ny = len(mtr_of_ims)\n",
        "    assert(ny != 0)\n",
        "\n",
        "    nx = len(mtr_of_ims[0])\n",
        "    assert(nx != 0)\n",
        "\n",
        "    im_sh = mtr_of_ims[0][0].shape\n",
        "    assert (2 <= len(im_sh) <= 3)\n",
        "    multichannel = len(im_sh) == 3\n",
        "\n",
        "    if multichannel:\n",
        "        h, w, c = im_sh\n",
        "    else:\n",
        "        h, w = im_sh\n",
        "\n",
        "    h_c = h * ny + spacing * (ny-1)\n",
        "    w_c = w * nx + spacing * (nx-1)\n",
        "\n",
        "    canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
        "    canvas = np.ones(shape=canv_sh, dtype=np.float32) * background_color\n",
        "\n",
        "    for iy, row in enumerate(mtr_of_ims):\n",
        "        y_ofs = iy * (h + spacing)\n",
        "        for ix, im in enumerate(row):\n",
        "            x_ofs = ix * (w + spacing)\n",
        "            canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
        "\n",
        "    return canvas\n",
        "\n",
        "def create_progress_mosaic(images_list, labels=None, title=\"Training Progress\", figsize=(15, 10)):\n",
        "    \"\"\"Create an enhanced mosaic showing training progress\"\"\"\n",
        "    mosaic_img = mosaic(images_list, spacing=3, background_color=0.9)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    im = ax.imshow(mosaic_img, cmap='gray', vmin=0, vmax=1)\n",
        "    \n",
        "    ax.set_title(title, fontsize=18, pad=20, fontweight='bold')\n",
        "    \n",
        "    # Add labels if provided\n",
        "    if labels is not None:\n",
        "        ax.set_ylabel(' | '.join(labels), fontsize=14, rotation=0, ha='right', va='center')\n",
        "    \n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "    cbar.set_label('Pixel Intensity', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHjhiIzRtAcf"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')  # use first available GPU\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_reconstruction_quality_analysis(model, samples, device):\n",
        "    \"\"\"\n",
        "    Analyze reconstruction quality across different digits and show failure cases\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        images = torch.tensor(samples['images']).to(device)\n",
        "        labels = samples['labels']\n",
        "        \n",
        "        reconstructions = model(images)\n",
        "        \n",
        "        # reconstruction errors\n",
        "        mse_errors = F.mse_loss(reconstructions, images, reduction='none')\n",
        "        mse_errors = mse_errors.view(len(images), -1).mean(dim=1).cpu().numpy()\n",
        "        \n",
        "        fig = plt.figure(figsize=(15, 3))\n",
        "        plt.title('Reconstruction Quality Analysis', fontsize=14)\n",
        "        \n",
        "        # error distribution by digit class\n",
        "        digit_errors = {}\n",
        "        for digit in range(10):\n",
        "            mask = labels == digit\n",
        "            digit_errors[digit] = mse_errors[mask]\n",
        "        \n",
        "        plt.boxplot([digit_errors[i] for i in range(10)], tick_labels=range(10))\n",
        "        plt.title('Reconstruction Error by Digit Class')\n",
        "        plt.xlabel('Digit Class')\n",
        "        plt.ylabel('MSE Error')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        best_indices = np.argsort(mse_errors)[:5]\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "        fig.suptitle('Best Reconstructions', fontsize=14)\n",
        "        \n",
        "        for i, idx in enumerate(best_indices):\n",
        "            orig_img = images[idx, 0].cpu().numpy()\n",
        "            recon_img = reconstructions[idx, 0].cpu().numpy()\n",
        "            comparison = np.hstack([orig_img, recon_img])\n",
        "            axes[i].imshow(comparison, cmap='gray')\n",
        "            axes[i].set_title(f'Digit: {labels[idx]}\\nError: {mse_errors[idx]:.4f}')\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        worst_indices = np.argsort(mse_errors)[-5:]\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "        fig.suptitle('Worst Reconstructions', fontsize=14)\n",
        "        \n",
        "        for i, idx in enumerate(worst_indices):\n",
        "            orig_img = images[idx, 0].cpu().numpy()\n",
        "            recon_img = reconstructions[idx, 0].cpu().numpy()\n",
        "            comparison = np.hstack([orig_img, recon_img])\n",
        "            axes[i].imshow(comparison, cmap='gray')\n",
        "            axes[i].set_title(f'Digit: {labels[idx]}\\nError: {mse_errors[idx]:.4f}')\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Architecture Visualization Functions\n",
        "\n",
        "def visualize_autoencoder_architecture(model, input_shape):\n",
        "    \"\"\"\n",
        "    Create a visual representation of the autoencoder architecture\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
        "    \n",
        "    # layer information\n",
        "    encoder_layers = []\n",
        "    decoder_layers = []\n",
        "    \n",
        "    # encoder\n",
        "    for name, layer in model.encoder.named_children():\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            encoder_layers.append(('Linear', layer.in_features, layer.out_features))\n",
        "        elif isinstance(layer, nn.Conv2d):\n",
        "            encoder_layers.append(('Conv2d', f'{layer.in_channels}ch ({layer.kernel_size[0]}×{layer.kernel_size[0]})k\\n', \n",
        "                                 f'{layer.out_channels}ch'))\n",
        "        elif isinstance(layer, nn.Flatten):\n",
        "            encoder_layers.append(('Flatten', '', ''))\n",
        "\n",
        "    # decoder\n",
        "    for name, layer in model.decoder.named_children():\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            decoder_layers.append(('Linear', layer.in_features, layer.out_features))\n",
        "        elif isinstance(layer, nn.ConvTranspose2d):\n",
        "            decoder_layers.append(('ConvTran2d', f'{layer.in_channels}ch', \n",
        "                                 f'\\n{layer.out_channels}ch ({layer.kernel_size[0]}×{layer.kernel_size[0]})k'))\n",
        "        elif isinstance(layer, nn.Unflatten):\n",
        "            decoder_layers.append(('Unflatten', '', ''))\n",
        "    \n",
        "    y_pos = 0.5\n",
        "    # positions for input + encoder + latent + decoder + output\n",
        "    total_positions = 1 + len(encoder_layers) + 1 + len(decoder_layers) + 1\n",
        "    x_positions = np.linspace(0.1, 0.9, total_positions)\n",
        "    \n",
        "    # box dimensions\n",
        "    box_width = 0.035\n",
        "    box_height = 0.15\n",
        "    latent_width = 0.045\n",
        "    latent_height = 0.2\n",
        "    \n",
        "    # Input\n",
        "    ax.add_patch(plt.Rectangle((x_positions[0]-box_width/2, y_pos-box_height/2), box_width, box_height, \n",
        "                              facecolor='lightblue', edgecolor='black'))\n",
        "    ax.text(x_positions[0], y_pos+box_height/2+0.05, 'Input\\n' + '×'.join(map(str, input_shape[1:])), \n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Encoder layers\n",
        "    for i, (layer_type, in_dim, out_dim) in enumerate(encoder_layers):\n",
        "        x = x_positions[i+1]\n",
        "        color = 'lightcoral' if 'Conv' in layer_type else 'lightgreen'\n",
        "        \n",
        "        ax.add_patch(plt.Rectangle((x-box_width/2, y_pos-box_height/2), box_width, box_height, \n",
        "                                  facecolor=color, edgecolor='black'))\n",
        "        ax.text(x, y_pos+box_height/2+0.05, f'{layer_type}\\n{in_dim} → {out_dim}', \n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # arrows\n",
        "        prev_x = x_positions[i]\n",
        "        arrow_start_x = prev_x + box_width/2\n",
        "        arrow_end_x = x - box_width/2\n",
        "        ax.arrow(arrow_start_x, y_pos, arrow_end_x - arrow_start_x - 0.008, 0,\n",
        "                head_width=0.025, head_length=0.008, fc='black', ec='black')\n",
        "    \n",
        "    # Latent space\n",
        "    bottleneck_x = x_positions[len(encoder_layers)+1]\n",
        "    ax.add_patch(plt.Rectangle((bottleneck_x-latent_width/2, y_pos-latent_height/2), latent_width, latent_height, \n",
        "                              facecolor='gold', edgecolor='red', linewidth=2))\n",
        "    ax.text(bottleneck_x, y_pos+latent_height/2+0.05, f'Latent\\n{model.code_size}D', \n",
        "            ha='center', va='bottom', fontsize=11, fontweight='bold', color='red')\n",
        "    \n",
        "    # arrows to latent space\n",
        "    prev_x = x_positions[len(encoder_layers)]\n",
        "    arrow_start_x = prev_x + box_width/2\n",
        "    arrow_end_x = bottleneck_x - latent_width/2\n",
        "    ax.arrow(arrow_start_x, y_pos, arrow_end_x - arrow_start_x - 0.008, 0,\n",
        "             head_width=0.025, head_length=0.008, fc='black', ec='black')\n",
        "    \n",
        "    # Decoder layers\n",
        "    for i, (layer_type, in_dim, out_dim) in enumerate(decoder_layers):\n",
        "        x = x_positions[len(encoder_layers) + 2 + i]\n",
        "        color = 'lightcoral' if 'Conv' in layer_type else 'lightgreen'\n",
        "        \n",
        "        ax.add_patch(plt.Rectangle((x-box_width/2, y_pos-box_height/2), box_width, box_height, \n",
        "                                  facecolor=color, edgecolor='black'))\n",
        "        ax.text(x, y_pos+box_height/2+0.05, f'{layer_type}\\n{in_dim} → {out_dim}', \n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # arrows\n",
        "        if i == 0:\n",
        "            # Arrow from latent space to first decoder layer\n",
        "            arrow_start_x = bottleneck_x + latent_width/2\n",
        "            arrow_end_x = x - box_width/2 - 0.008\n",
        "        else:\n",
        "            # Arrow from previous decoder layer\n",
        "            prev_x = x_positions[len(encoder_layers) + 1 + i]\n",
        "            arrow_start_x = prev_x + box_width/2\n",
        "            arrow_end_x = x - box_width/2 - 0.008\n",
        "            \n",
        "        ax.arrow(arrow_start_x, y_pos, arrow_end_x - arrow_start_x, 0,\n",
        "                head_width=0.025, head_length=0.008, fc='black', ec='black')\n",
        "    \n",
        "    # Output\n",
        "    output_x = x_positions[-1]\n",
        "    ax.add_patch(plt.Rectangle((output_x-box_width/2, y_pos-box_height/2), box_width, box_height, \n",
        "                              facecolor='lightblue', edgecolor='black'))\n",
        "    ax.text(output_x, y_pos+box_height/2+0.05, 'Output\\n' + '×'.join(map(str, input_shape[1:])), \n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Final arrow from last decoder layer to output\n",
        "    prev_x = x_positions[-2]\n",
        "    arrow_start_x = prev_x + box_width/2\n",
        "    arrow_end_x = output_x - box_width/2 - 0.008\n",
        "    ax.arrow(arrow_start_x, y_pos, arrow_end_x - arrow_start_x, 0,\n",
        "            head_width=0.025, head_length=0.008, fc='black', ec='black')\n",
        "    \n",
        "    # labels\n",
        "    ax.text(0.3, 0.1, 'ENCODER\\n(Compression)', ha='center', va='center', \n",
        "            fontsize=14, fontweight='bold', color='darkred',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='darkred'))\n",
        "    \n",
        "    ax.text(0.7, 0.1, 'DECODER\\n(Reconstruction)', ha='center', va='center', \n",
        "            fontsize=14, fontweight='bold', color='darkblue',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='darkblue'))\n",
        "    \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title('Autoencoder Architecture', fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_information_flow_diagram():\n",
        "    \"\"\"\n",
        "    Create a conceptual diagram showing information flow and dimensionality\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
        "    \n",
        "    # Define the flow stages\n",
        "    stages = [\n",
        "        ('Raw Image', 784, 'lightblue'),\n",
        "        ('Hidden Layer', 128, 'lightgreen'), \n",
        "        ('Latent Code', 5, 'gold'),\n",
        "        ('Hidden Layer', 128, 'lightgreen'),\n",
        "        ('Reconstructed', 784, 'lightcoral')\n",
        "    ]\n",
        "    \n",
        "    x_positions = np.linspace(0.1, 0.9, len(stages))\n",
        "    \n",
        "    for i, (name, size, color) in enumerate(stages):\n",
        "        x = x_positions[i]\n",
        "        \n",
        "        # Draw rectangle with height proportional to dimensionality\n",
        "        height = 0.1 + (size / 1000) * 0.6  # Scale height\n",
        "        width = 0.08\n",
        "        \n",
        "        rect = plt.Rectangle((x - width/2, 0.5 - height/2), width, height, \n",
        "                           facecolor=color, edgecolor='black', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        \n",
        "        # Add labels\n",
        "        ax.text(x, 0.5 + height/2 + 0.05, name, ha='center', va='bottom', \n",
        "                fontsize=12, fontweight='bold')\n",
        "        ax.text(x, 0.5, str(size), ha='center', va='center', \n",
        "                fontsize=14, fontweight='bold')\n",
        "        \n",
        "        # Add arrows\n",
        "        if i < len(stages) - 1:\n",
        "            arrow_start = x + width/2\n",
        "            arrow_end = x_positions[i+1] - width/2\n",
        "            ax.arrow(arrow_start, 0.5, arrow_end - arrow_start, 0,\n",
        "                    head_width=0.03, head_length=0.02, fc='black', ec='black')\n",
        "    \n",
        "    # Add compression/expansion labels\n",
        "    ax.text(0.25, 0.15, 'COMPRESSION\\\\n(Information Bottleneck)', ha='center', va='center',\n",
        "            fontsize=11, fontweight='bold', color='red',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='red'))\n",
        "    \n",
        "    ax.text(0.75, 0.15, 'EXPANSION\\\\n(Information Recovery)', ha='center', va='center',\n",
        "            fontsize=11, fontweight='bold', color='blue',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='blue'))\n",
        "    \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title('Information Flow: From 784 Dimensions to 5 and Back', \n",
        "                fontsize=16, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latent Space Exploration Functions\n",
        "\n",
        "def visualize_latent_interpolation_grid(model, device, latent_dim=2, n_steps=10, range_vals=(-3, 3)):\n",
        "    \"\"\"\n",
        "    Create a comprehensive grid showing what happens when we interpolate in latent space\n",
        "    \"\"\"\n",
        "    if latent_dim < 2:\n",
        "        print(f\"Need at least 2 latent dimensions for grid interpolation, got {latent_dim}\")\n",
        "        return\n",
        "        \n",
        "    model.eval()\n",
        "    \n",
        "    # Create a grid of latent values\n",
        "    x_vals = np.linspace(range_vals[0], range_vals[1], n_steps)\n",
        "    y_vals = np.linspace(range_vals[0], range_vals[1], n_steps)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_steps, n_steps, figsize=(15, 15))\n",
        "    fig.suptitle('Latent Space Grid Exploration\\\\n'\n",
        "                f'Each image corresponds to a point (x,y) in the {latent_dim}D latent space', \n",
        "                fontsize=16, fontweight='bold')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, y_val in enumerate(reversed(y_vals)):  # Reverse for correct orientation\n",
        "            for j, x_val in enumerate(x_vals):\n",
        "                # Create latent vector\n",
        "                z = torch.zeros(1, latent_dim).to(device)\n",
        "                z[0, 0] = x_val\n",
        "                z[0, 1] = y_val\n",
        "                \n",
        "                # Generate image\n",
        "                generated = model.decode(z)\n",
        "                img = to_np_showable(generated[0])\n",
        "                \n",
        "                # Plot\n",
        "                axes[i, j].imshow(img, cmap='gray')\n",
        "                axes[i, j].axis('off')\n",
        "                \n",
        "                # Add coordinates as title for corner images\n",
        "                if (i == 0 and j == 0) or (i == 0 and j == n_steps-1) or \\\n",
        "                   (i == n_steps-1 and j == 0) or (i == n_steps-1 and j == n_steps-1):\n",
        "                    axes[i, j].set_title(f'({x_val:.1f},{y_val:.1f})', fontsize=8)\n",
        "    \n",
        "    # Add axis labels\n",
        "    fig.text(0.5, 0.02, f'Latent Dimension 1 → ({range_vals[0]} to {range_vals[1]})', \n",
        "             ha='center', fontsize=12)\n",
        "    fig.text(0.02, 0.5, f'Latent Dimension 2 → ({range_vals[0]} to {range_vals[1]})', \n",
        "             va='center', rotation='vertical', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_digit_interpolation(model, samples, device, digit1=3, digit2=8, n_steps=8):\n",
        "    \"\"\"\n",
        "    Show interpolation between two specific digits in latent space\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Find examples of the two digits\n",
        "    labels = samples['labels']\n",
        "    images = samples['images']\n",
        "    \n",
        "    # Get indices for each digit\n",
        "    digit1_indices = np.where(labels == digit1)[0]\n",
        "    digit2_indices = np.where(labels == digit2)[0]\n",
        "    \n",
        "    if len(digit1_indices) == 0 or len(digit2_indices) == 0:\n",
        "        print(f\"Could not find examples of digits {digit1} and/or {digit2}\")\n",
        "        return\n",
        "    \n",
        "    # Select one example of each digit\n",
        "    idx1 = digit1_indices[0]\n",
        "    idx2 = digit2_indices[0]\n",
        "    \n",
        "    img1 = torch.tensor(images[idx1:idx1+1]).to(device)\n",
        "    img2 = torch.tensor(images[idx2:idx2+1]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get latent representations\n",
        "        z1 = model.encode(img1)\n",
        "        z2 = model.encode(img2)\n",
        "        \n",
        "        # Create interpolation\n",
        "        alphas = np.linspace(0, 1, n_steps)\n",
        "        interpolated_images = []\n",
        "        \n",
        "        for alpha in alphas:\n",
        "            # Interpolate in latent space\n",
        "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
        "            \n",
        "            # Decode\n",
        "            img_interp = model.decode(z_interp)\n",
        "            interpolated_images.append(to_np_showable(img_interp[0]))\n",
        "        \n",
        "        # Visualize\n",
        "        fig, axes = plt.subplots(2, n_steps, figsize=(2*n_steps, 4))\n",
        "        fig.suptitle(f'Interpolating Between Digit {digit1} and Digit {digit2} in Latent Space', \n",
        "                    fontsize=14, fontweight='bold')\n",
        "        \n",
        "        # Show interpolated images\n",
        "        for i, (img, alpha) in enumerate(zip(interpolated_images, alphas)):\n",
        "            axes[0, i].imshow(img, cmap='gray')\n",
        "            axes[0, i].set_title(f'α={alpha:.2f}', fontsize=10)\n",
        "            axes[0, i].axis('off')\n",
        "        \n",
        "        # Show latent space positions\n",
        "        z1_np = z1.cpu().numpy()[0]\n",
        "        z2_np = z2.cpu().numpy()[0]\n",
        "        \n",
        "        if len(z1_np) >= 2:  # If we have at least 2D latent space\n",
        "            for i, alpha in enumerate(alphas):\n",
        "                z_interp_np = (1 - alpha) * z1_np + alpha * z2_np\n",
        "                \n",
        "                # Plot latent space with current interpolation point\n",
        "                axes[1, i].scatter(z1_np[0], z1_np[1], c='blue', s=100, marker='s', \n",
        "                                 label=f'Digit {digit1}' if i == 0 else \"\")\n",
        "                axes[1, i].scatter(z2_np[0], z2_np[1], c='red', s=100, marker='s',\n",
        "                                 label=f'Digit {digit2}' if i == 0 else \"\")\n",
        "                axes[1, i].scatter(z_interp_np[0], z_interp_np[1], c='green', s=80, marker='o')\n",
        "                \n",
        "                # Draw interpolation line\n",
        "                axes[1, i].plot([z1_np[0], z2_np[0]], [z1_np[1], z2_np[1]], \n",
        "                               'k--', alpha=0.5, linewidth=1)\n",
        "                \n",
        "                axes[1, i].set_xlim(min(z1_np[0], z2_np[0]) - 1, max(z1_np[0], z2_np[0]) + 1)\n",
        "                axes[1, i].set_ylim(min(z1_np[1], z2_np[1]) - 1, max(z1_np[1], z2_np[1]) + 1)\n",
        "                axes[1, i].set_title(f'Latent Position\\\\nα={alpha:.2f}', fontsize=10)\n",
        "                axes[1, i].grid(True, alpha=0.3)\n",
        "                \n",
        "                if i == 0:\n",
        "                    axes[1, i].legend(fontsize=8)\n",
        "        else:\n",
        "            # For 1D or higher dimensional latent spaces, show bar plots\n",
        "            for i, alpha in enumerate(alphas):\n",
        "                z_interp_np = (1 - alpha) * z1_np + alpha * z2_np\n",
        "                axes[1, i].bar(range(len(z_interp_np)), z_interp_np, alpha=0.7)\n",
        "                axes[1, i].set_title(f'Latent Code\\\\nα={alpha:.2f}', fontsize=10)\n",
        "                axes[1, i].set_ylim(-2, 2)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def analyze_latent_dimensions_importance(model, samples, device, n_dims_to_test=5):\n",
        "    \"\"\"\n",
        "    Analyze which latent dimensions are most important by zeroing them out\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Get a few sample images\n",
        "    sample_indices = np.random.choice(len(samples['images']), 5, replace=False)\n",
        "    test_images = torch.tensor(samples['images'][sample_indices]).to(device)\n",
        "    test_labels = samples['labels'][sample_indices]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get original latent codes\n",
        "        original_latents = model.encode(test_images)\n",
        "        original_reconstructions = model.decode(original_latents)\n",
        "        \n",
        "        # Test importance of each dimension\n",
        "        latent_dim = original_latents.shape[1]\n",
        "        importance_scores = np.zeros(latent_dim)\n",
        "        \n",
        "        for dim in range(min(latent_dim, n_dims_to_test)):\n",
        "            # Zero out this dimension\n",
        "            modified_latents = original_latents.clone()\n",
        "            modified_latents[:, dim] = 0\n",
        "            \n",
        "            # Reconstruct\n",
        "            modified_reconstructions = model.decode(modified_latents)\n",
        "            \n",
        "            # Calculate reconstruction error increase\n",
        "            original_error = F.mse_loss(original_reconstructions, test_images, reduction='mean')\n",
        "            modified_error = F.mse_loss(modified_reconstructions, test_images, reduction='mean')\n",
        "            \n",
        "            importance_scores[dim] = (modified_error - original_error).item()\n",
        "        \n",
        "        # Visualize results\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        fig.suptitle('Analyzing Latent Dimension Importance', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Show importance scores\n",
        "        axes[0, 0].bar(range(min(latent_dim, n_dims_to_test)), \n",
        "                      importance_scores[:min(latent_dim, n_dims_to_test)])\n",
        "        axes[0, 0].set_title('Reconstruction Error Increase\\\\nWhen Dimension is Zeroed')\n",
        "        axes[0, 0].set_xlabel('Latent Dimension')\n",
        "        axes[0, 0].set_ylabel('Error Increase')\n",
        "        \n",
        "        # 2. Show original latent values distribution\n",
        "        latent_values = original_latents.cpu().numpy()\n",
        "        for dim in range(min(latent_dim, 5)):\n",
        "            axes[0, 1].hist(latent_values[:, dim], alpha=0.7, \n",
        "                           label=f'Dim {dim}', bins=10)\n",
        "        axes[0, 1].set_title('Distribution of Latent Values')\n",
        "        axes[0, 1].set_xlabel('Activation Value')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "        axes[0, 1].legend()\n",
        "        \n",
        "        # 3. Show variance per dimension\n",
        "        variances = np.var(latent_values, axis=0)\n",
        "        axes[0, 2].bar(range(min(latent_dim, n_dims_to_test)), \n",
        "                      variances[:min(latent_dim, n_dims_to_test)])\n",
        "        axes[0, 2].set_title('Variance per Latent Dimension')\n",
        "        axes[0, 2].set_xlabel('Latent Dimension')\n",
        "        axes[0, 2].set_ylabel('Variance')\n",
        "        \n",
        "        # 4-6. Show examples of reconstruction with most important dimension zeroed\n",
        "        most_important_dim = np.argmax(importance_scores[:min(latent_dim, n_dims_to_test)])\n",
        "        \n",
        "        for i in range(3):\n",
        "            if i < len(test_images):\n",
        "                # Original\n",
        "                orig_img = to_np_showable(test_images[i])\n",
        "                orig_recon = to_np_showable(original_reconstructions[i])\n",
        "                \n",
        "                # With important dimension zeroed\n",
        "                modified_latents = original_latents[i:i+1].clone()\n",
        "                modified_latents[:, most_important_dim] = 0\n",
        "                modified_recon = model.decode(modified_latents)\n",
        "                modified_recon_img = to_np_showable(modified_recon[0])\n",
        "                \n",
        "                # Show comparison\n",
        "                comparison = np.hstack([orig_img, orig_recon, modified_recon_img])\n",
        "                axes[1, i].imshow(comparison, cmap='gray')\n",
        "                axes[1, i].set_title(f'Digit {test_labels[i]}: Original | Normal Recon | Dim {most_important_dim} Zeroed')\n",
        "                axes[1, i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5NaXhkctAcg"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "print(f'device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGM4mTqJ0htE"
      },
      "source": [
        "# Setup\n",
        "\n",
        "<div>\n",
        "<h3 style=\"margin-top: 0;\"> What are Autoencoders?</h3>\n",
        "\n",
        "**Autoencoders** are neural networks designed to learn efficient representations of data by:\n",
        "- **Encoding**: Compressing input data into a lower-dimensional latent space\n",
        "- **Decoding**: Reconstructing the original data from the latent representation\n",
        "\n",
        "The key insight: *If we can compress and perfectly reconstruct data, the compressed representation must capture the essential features.*\n",
        "\n",
        "$$\\text{Input} \\xrightarrow{\\text{Encoder}} \\text{Latent Space} \\xrightarrow{\\text{Decoder}} \\text{Reconstruction}$$\n",
        "\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<h3 style=\"margin-top: 0;\"> Applications</h3>\n",
        "<ul>\n",
        "<li><strong>Dimensionality Reduction:</strong> Learning compact representations</li>\n",
        "<li><strong>Denoising:</strong> Removing noise while preserving important features</li>\n",
        "<li><strong>Anomaly Detection:</strong> Identifying outliers based on reconstruction error</li>\n",
        "<li><strong>Data Generation:</strong> Creating new samples by sampling from latent space</li>\n",
        "</ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3xcUuod8bUt"
      },
      "source": [
        "## Dataset Loading and Preprocessing\n",
        "\n",
        "<div>\n",
        "<h4 style=\"margin-top: 0;\"> MNIST Dataset</h4>\n",
        "We'll start with the MNIST dataset because:\n",
        "<ul>\n",
        "<li><strong>Visual Interpretability:</strong> Easy to judge reconstruction quality</li>\n",
        "<li><strong>Simplicity:</strong> 28×28 grayscale images of handwritten digits</li>\n",
        "<li><strong>Low Complexity:</strong> Perfect for understanding autoencoder concepts</li>\n",
        "</ul>\n",
        "\n",
        "**Preprocessing Steps:**\n",
        "1. Convert to tensors and normalize to [-1, 1] range\n",
        "2. Add controllable noise for denoising experiments\n",
        "3. Create data loaders with custom collation function\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWqy2uMffPA"
      },
      "source": [
        "Lets start with a simple, well understood mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSU2wVsXtAcj"
      },
      "outputs": [],
      "source": [
        "NOISE_RATE = 0.1\n",
        "N_SAMPLE = 32\n",
        "N_VIS_SAMPLE = 2\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSPzUDchtAck"
      },
      "outputs": [],
      "source": [
        "def collate_ae_dataset(samples, noise_rate=0.0, data_range=None):\n",
        "    \"\"\"\n",
        "    The function collates samples into a batch, and creates noisy samples if DENOISING is True\n",
        "    for the denoising autoencoder.\n",
        "    \n",
        "    Args:\n",
        "        samples: List of (image, label) tuples\n",
        "        noise_rate: Float, probability of corrupting each pixel (0.0 = no noise)\n",
        "        data_range: tuple (min_val, max_val) for noise generation\n",
        "                   If None, auto-detect from normalization parameters\n",
        "    \"\"\"\n",
        "    xs = [s[0] for s in samples]\n",
        "    ys = [s[1] for s in samples]\n",
        "    xs = torch.stack(xs)\n",
        "    ys = torch.concat(ys)\n",
        "\n",
        "    add_noise = noise_rate > 0.\n",
        "    if add_noise:\n",
        "        sh = xs.shape\n",
        "        noise_mask = torch.bernoulli(torch.full(sh, noise_rate))\n",
        "        if data_range is None:\n",
        "            # Auto-detect range from data\n",
        "            noise_min = float(xs.min())\n",
        "            noise_max = float(xs.max())\n",
        "        else:\n",
        "            noise_min, noise_max = data_range\n",
        "        \n",
        "        # Generate salt-and-pepper noise\n",
        "        sp_noise = torch.bernoulli(torch.full(sh, 0.5)) * (noise_max - noise_min) + noise_min\n",
        "        \n",
        "        xns = xs * (1-noise_mask) + sp_noise*noise_mask\n",
        "    else:\n",
        "        xns = xs\n",
        "\n",
        "    return xns.to(device), xs.to(device), ys.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilgX4HUUtAcl"
      },
      "outputs": [],
      "source": [
        "# m, s, DATA_RANGE = 0.5, 1. , (-0.5, 0.5)\n",
        "m, s, DATA_RANGE = 0.5, 0.5, (-1, 1)\n",
        "# m, s, DATA_RANGE = 0., 1., (0, 1)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Pad(2), # to make images 32x32\n",
        "    transforms.Normalize((m,), (s,))\n",
        "])\n",
        "\n",
        "lable_transform = transforms.Compose([lambda x:torch.LongTensor([x])])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform, target_transform=lable_transform)\n",
        "valid_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform, target_transform=lable_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J00NBF750htG"
      },
      "outputs": [],
      "source": [
        "for sample in valid_dataset:\n",
        "    img, label = sample\n",
        "    print(type(img), img.shape)\n",
        "    print(type(label), label.shape)\n",
        "    print(img.flatten().shape)\n",
        "    plt.hist(img.flatten(), bins=100)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13sBIxAS0htG"
      },
      "outputs": [],
      "source": [
        "plt.hist(valid_dataset.data.numpy().flatten(), bins=100);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOISE_RATE = 0.1\n",
        "DATA_RANGE = (-1, 1)\n",
        "\n",
        "# customized collation function\n",
        "collate_fn = partial(collate_ae_dataset, noise_rate=NOISE_RATE, data_range=DATA_RANGE)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrjwOhvC0htG"
      },
      "outputs": [],
      "source": [
        "# Iteration on the dataloader\n",
        "for batch_idx, (noisy_imgs, imgs, labels) in enumerate(valid_loader):\n",
        "    print(\"Batch idx:\", batch_idx)\n",
        "    print(\"Noisy imgs:\", \"\\ttype:\", type(noisy_imgs), \"\\tlen:\", len(noisy_imgs), \"\\tshape:\", noisy_imgs.shape)\n",
        "    print(\"Imgs:\", \"\\t\\ttype:\", type(imgs), \"\\tlen:\", len(imgs), \"\\tshape:\", imgs.shape)\n",
        "    print(\"Labels:\", \"\\ttype:\", type(labels), \"\\tlen:\", len(labels), \"\\tshape:\", labels.shape)\n",
        "    plt.hist(imgs.flatten().cpu().numpy(), bins=100);\n",
        "    plt.hist(noisy_imgs.flatten().cpu().numpy(), bins=100, alpha=0.5);\n",
        "    plt.yscale('log')\n",
        "    plt.legend(['clean', 'noisy'])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sZyi9IPtAcr"
      },
      "outputs": [],
      "source": [
        "for s in train_loader:\n",
        "  xns, xs, ys = s\n",
        "  print(xns.shape, xs.shape, ys.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIPvKCH9tAcs"
      },
      "outputs": [],
      "source": [
        "plot_im_samples(xns, is_torch=True)\n",
        "plot_im_samples(xs, is_torch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mMqQFO_tAcs"
      },
      "outputs": [],
      "source": [
        "# fill array of all preprocessed training samples, converted to numpy:\n",
        "train_images = []\n",
        "for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "    train_images.append(data.detach().cpu().numpy())\n",
        "\n",
        "train_images = np.concatenate(train_images, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmN0NguPtAc3"
      },
      "outputs": [],
      "source": [
        "print (\"train_images.shape = \", train_images.shape)\n",
        "print (\"train_images.dtype = \", train_images.dtype)\n",
        "print (\"train_images.mean/std() = \", get_mean_std(train_images))\n",
        "print (\"train_images.min/max() = \", get_min_max(train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uafPhG4EtAc4"
      },
      "outputs": [],
      "source": [
        "plt.hist(train_images.flatten(), bins=100, log=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta5guwIstAc4"
      },
      "outputs": [],
      "source": [
        "del train_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8kScAZTtAc5"
      },
      "source": [
        "We will also prepare a subsampled dataset from the validation set for the visualisation purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koccayybtAc5"
      },
      "outputs": [],
      "source": [
        "def get_samples(valid_loader):\n",
        "  # 1. get numpy array of all validation images:\n",
        "  val_images_noisy = []\n",
        "  val_images = []\n",
        "  val_labels = []\n",
        "\n",
        "  for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "      val_images_noisy.append(noisy_data.detach().cpu().numpy())\n",
        "      val_images.append(data.detach().cpu().numpy())\n",
        "      val_labels.append(target.detach().cpu().numpy())\n",
        "\n",
        "  val_images_noisy = np.concatenate(val_images_noisy, axis=0)\n",
        "  val_images = np.concatenate(val_images, axis=0)\n",
        "  val_labels = np.concatenate(val_labels, axis=0)\n",
        "\n",
        "  # 2. get numpy array of balanced validation samples for visualization:\n",
        "  sample_images_noisy = []\n",
        "  sample_images = []\n",
        "  sample_labels = []\n",
        "  single_el_idx = []  # indexes of single element per class for visualization\n",
        "\n",
        "  n_class = np.max(val_labels) + 1\n",
        "  for class_idx in range(n_class):\n",
        "    map_c = val_labels == class_idx\n",
        "\n",
        "    ims_c_noisy = val_images_noisy[map_c]\n",
        "    ims_c = val_images[map_c]\n",
        "\n",
        "    samples_idx = np.random.choice(len(ims_c), N_SAMPLE, replace=False)\n",
        "\n",
        "    ims_c_noisy_samples = ims_c_noisy[samples_idx]\n",
        "    ims_c_samples = ims_c[samples_idx]\n",
        "\n",
        "    sample_images_noisy.append(ims_c_noisy_samples)\n",
        "    sample_images.append(ims_c_samples)\n",
        "\n",
        "    sample_labels.append([class_idx]*N_SAMPLE)\n",
        "\n",
        "    start_idx = N_SAMPLE*class_idx\n",
        "    single_el_idx.extend([start_idx + i for i in range(min(N_VIS_SAMPLE, N_SAMPLE))])\n",
        "\n",
        "  sample_images_noisy = np.concatenate(sample_images_noisy, axis=0)\n",
        "  sample_images = np.concatenate(sample_images, axis=0)\n",
        "  sample_labels = np.concatenate(sample_labels, axis=0)\n",
        "  single_el_idx = np.array(single_el_idx)\n",
        "\n",
        "  samples = {\n",
        "      'images_noisy': sample_images_noisy,\n",
        "      'images': sample_images,\n",
        "      'labels': sample_labels,\n",
        "      'single_el_idx': single_el_idx\n",
        "\n",
        "  }\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFJ_pQgrtAc6"
      },
      "outputs": [],
      "source": [
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYAEkBDKtAc6"
      },
      "outputs": [],
      "source": [
        "samples['images'].shape, samples['labels'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F94TFGHtAc6"
      },
      "outputs": [],
      "source": [
        "single_el_idx = samples['single_el_idx']\n",
        "plot_im_samples(samples['images_noisy'][single_el_idx, 0], n=20, is_torch=False)\n",
        "plot_im_samples(samples['images'][single_el_idx, 0], n=20, is_torch=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbR9emfD8d41"
      },
      "source": [
        "## Helper Autoencoder Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNM2dEZOh5EN"
      },
      "source": [
        "We will start from implementing an Autoencoder model base class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGs_gV5GtAc8"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_size, code_size):\n",
        "        self.input_size = list(input_size)  # shape of data sample\n",
        "        self.flat_data_size = np.prod(self.input_size)\n",
        "        self.hidden_size = 128\n",
        "\n",
        "        self.code_size = code_size  # code size\n",
        "\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(), # C x H x W = 1 x 28 x 28 = 784 vector\n",
        "\n",
        "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, self.code_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.code_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
        "            nn.Tanh(),  # Think: why tanh?\n",
        "\n",
        "            nn.Unflatten(1, self.input_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_z=False):\n",
        "        encoded = self.encode(x)\n",
        "        decoded = self.decode(encoded)\n",
        "        return (decoded, encoded) if return_z else decoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)*1.1\n",
        "\n",
        "    def get_n_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def eval_on_samples(ae_model, epoch, samples):\n",
        "    # this is called on end of each training epoch\n",
        "    xns = samples['images_noisy']\n",
        "    xns = torch.tensor(xns, dtype=torch.float32).to(device)\n",
        "    #labels = samples['labels']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        yz = ae_model(xns, return_z=True)\n",
        "        yz = [el.detach().cpu().numpy() for el in yz]\n",
        "\n",
        "        y = yz[0]\n",
        "        z = yz[1:]\n",
        "\n",
        "    res = {'z': z, 'y': y, 'epoch': epoch}\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoITlVuntAc8"
      },
      "outputs": [],
      "source": [
        "# Training Visualization Functions\n",
        "\n",
        "def plot_hist(history, logscale=True, figsize=(12, 6)):\n",
        "    \"\"\"\n",
        "    Plot training history with enhanced styling\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "    \n",
        "    loss = history['loss']\n",
        "    v_loss = history['val_loss']\n",
        "    epochs = history['epoch']\n",
        "    \n",
        "    # Plot training curves\n",
        "    plot_func = ax1.semilogy if logscale else ax1.plot\n",
        "    \n",
        "    line1 = plot_func(epochs, loss, label='Training Loss', \n",
        "                     color=COLORS['encoder'], linewidth=3, alpha=0.8)\n",
        "    line2 = plot_func(epochs, v_loss, label='Validation Loss', \n",
        "                     color=COLORS['decoder'], linewidth=3, alpha=0.8)\n",
        "    \n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training Progress', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add improvement metrics\n",
        "    initial_train_loss = loss[0]\n",
        "    final_train_loss = loss[-1]\n",
        "    initial_val_loss = v_loss[0]\n",
        "    final_val_loss = v_loss[-1]\n",
        "    \n",
        "    train_improvement = ((initial_train_loss - final_train_loss) / initial_train_loss) * 100\n",
        "    val_improvement = ((initial_val_loss - final_val_loss) / initial_val_loss) * 100\n",
        "    \n",
        "    # Plot loss difference\n",
        "    loss_diff = np.array(v_loss) - np.array(loss)\n",
        "    ax2.plot(epochs, loss_diff, color=COLORS['latent'], linewidth=2)\n",
        "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Validation - Training Loss', fontsize=12)\n",
        "    ax2.set_title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=1.5, max_epochs=None):\n",
        "    \"\"\"\n",
        "    Enhanced visualization of sample reconstruction evolution\n",
        "    \"\"\"\n",
        "    single_el_idx = samples['single_el_idx']\n",
        "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
        "    images = samples['images'][single_el_idx, 0]\n",
        "    \n",
        "    epochs_to_show = sorted([ep for ep in sample_history.keys() \n",
        "                           if ep % epoch_stride == 0 or ep == max(sample_history.keys())])\n",
        "    \n",
        "    if max_epochs:\n",
        "        epochs_to_show = epochs_to_show[:max_epochs]\n",
        "    \n",
        "    for epoch_idx in epochs_to_show:\n",
        "        hist_el = sample_history[epoch_idx]\n",
        "        \n",
        "        # Create three rows: noisy input, reconstruction, original\n",
        "        samples_arr = [\n",
        "            images_noisy,\n",
        "            hist_el['y'][single_el_idx, 0], \n",
        "            images\n",
        "        ]\n",
        "        \n",
        "        row_labels = ['Noisy Input', 'Reconstruction', 'Ground Truth']\n",
        "        \n",
        "        # Create enhanced mosaic\n",
        "        fig, ax = plt.subplots(figsize=(fig_scale * len(images_noisy), fig_scale * 3))\n",
        "        \n",
        "        mosaic_img = mosaic(samples_arr, spacing=2)\n",
        "        im = ax.imshow(mosaic_img, cmap='gray', vmin=-0.5, vmax=0.5)\n",
        "        \n",
        "        # Add title and labels\n",
        "        ax.set_title(f'Reconstruction Progress - Epoch {int(epoch_idx)}', \n",
        "                    fontsize=16, fontweight='bold', pad=20)\n",
        "        \n",
        "        # Add row labels\n",
        "        ax.set_ylabel(' | '.join(row_labels), fontsize=12, rotation=0, ha='right')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, shrink=0.6, aspect=30)\n",
        "        cbar.set_label('Pixel Value', fontsize=10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def plot_latent_evolution(sample_history, samples, figsize=(15, 10)):\n",
        "    \"\"\"\n",
        "    Visualize how latent space representations evolve during training\n",
        "    \"\"\"\n",
        "    labels = samples['labels']\n",
        "    epochs = sorted(sample_history.keys())\n",
        "    \n",
        "    # Select key epochs to show\n",
        "    key_epochs = [epochs[0], epochs[len(epochs)//4], epochs[len(epochs)//2], \n",
        "                  epochs[3*len(epochs)//4], epochs[-1]]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, len(key_epochs), figsize=figsize)\n",
        "    fig.suptitle('Latent Space Evolution', fontsize=18, fontweight='bold')\n",
        "    \n",
        "    for i, epoch in enumerate(key_epochs):\n",
        "        z = sample_history[epoch]['z'][0]  # Get latent representations\n",
        "        \n",
        "        scatter = axes[i].scatter(z[:, 0], z[:, 1], c=labels, \n",
        "                                cmap='tab10', s=30, alpha=0.7)\n",
        "        axes[i].set_title(f'Epoch {epoch}', fontsize=14)\n",
        "        axes[i].set_xlabel('Latent Dim 1', fontsize=10)\n",
        "        axes[i].set_ylabel('Latent Dim 2', fontsize=10)\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Set consistent axis limits\n",
        "        axes[i].set_xlim(-6, 6)\n",
        "        axes[i].set_ylim(-6, 6)\n",
        "    \n",
        "    # Add colorbar for digit labels\n",
        "    cbar = plt.colorbar(scatter, ax=axes[-1], shrink=0.8)\n",
        "    cbar.set_label('Digit Class', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwnutofatAc9"
      },
      "outputs": [],
      "source": [
        "def run_on_trained(model, root_dir, run_fn, device, ep=None, model_filename=None):\n",
        "    \"\"\"\n",
        "    Helper function to excecute any function on model in state after `ep` training epoch\n",
        "    \"\"\"\n",
        "\n",
        "    if model_filename is None:\n",
        "        if ep is not None:\n",
        "            model_filename = root_dir/f'model_{ep:03d}.pth'\n",
        "        else:\n",
        "            model_filename = sorted(list(root_dir.glob('*.pth')))[-1]  # last model state\n",
        "    if device == torch.device('cpu'):\n",
        "        model_dict = torch.load(model_filename, map_location='cpu', weights_only=False)\n",
        "    else:\n",
        "        model_dict = torch.load(model_filename, weights_only=False)\n",
        "\n",
        "    model.load_state_dict(model_dict['model_state_dict'])\n",
        "\n",
        "    run_fn(model)\n",
        "\n",
        "def run_on_all_training_history(model, root_dir, run_fn, device, n_ep=None):\n",
        "    \"\"\"\n",
        "    Helper function to excecute any function on model state after each of the training epochs\n",
        "    \"\"\"\n",
        "    if n_ep is not None:\n",
        "        for ep in range(n_ep):\n",
        "            print(f'running on epoch {ep+1}/{n_ep}...')\n",
        "            run_on_trained(model, root_dir, run_fn, device, ep=ep)\n",
        "    else:\n",
        "        for model_filename in sorted(root_dir.glob('*.pth')):\n",
        "            print(f'running on checkpoint {model_filename}...')\n",
        "            run_on_trained(model, root_dir, run_fn, device, model_filename=model_filename)\n",
        "    print(f'done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evn49kSItAc9"
      },
      "outputs": [],
      "source": [
        "train_batch = next(iter(train_loader))\n",
        "xns, xs, ys = train_batch\n",
        "print('sample shapes:', xns.shape, xs.shape, ys.shape)\n",
        "in_size = xns.shape[1:]\n",
        "\n",
        "ae = AutoEncoder(input_size=in_size, code_size=5).to(device)\n",
        "y = ae(xns)\n",
        "print('output shape:', y.shape)\n",
        "plot_im_samples(xns, is_torch=True)\n",
        "plot_im_samples(y, is_torch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVMY2jaA62tN"
      },
      "outputs": [],
      "source": [
        "x = xns[0]# - y[1]\n",
        "d = y[0]# - y[1]\n",
        "\n",
        "im0 = x[0].detach().cpu().numpy()\n",
        "im1 = d[0].detach().cpu().numpy()\n",
        "\n",
        "# plt.imshow(im, cmap='gray', vmin=-1, vmax=1)\n",
        "bins = np.linspace(-1, 1, 100)\n",
        "plt.hist(im0.flatten(), bins, alpha=0.3);\n",
        "plt.hist(im1.flatten(), bins, alpha=0.3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4i9tkmotAc_"
      },
      "outputs": [],
      "source": [
        "ae.get_n_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Understanding the Architecture\n",
        "\n",
        "# Visualize the autoencoder architecture\n",
        "print(\"Autoencoder Architecture Visualization:\")\n",
        "visualize_autoencoder_architecture(ae, in_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUG1JVSCtAc_"
      },
      "source": [
        "# Simple Autoencoder\n",
        "\n",
        "<div>\n",
        "\n",
        "<h3> Architecture Overview</h3>\n",
        "\n",
        "Our first autoencoder follows a simple **Encoder-Decoder** architecture:\n",
        "\n",
        "**Encoder Path (Dimensionality Reduction):**\n",
        "- Input: 28×28 = 784 pixels\n",
        "- Hidden Layer: 128 neurons + ReLU\n",
        "- **Bottleneck**: 5 neurons + Sigmoid (our latent space!)\n",
        "\n",
        "**Decoder Path (Reconstruction):**\n",
        "- Latent: 5 neurons \n",
        "- Hidden Layer: 128 neurons + ReLU\n",
        "- Output: 784 pixels + Tanh → reshape to 28×28\n",
        "\n",
        "<div>\n",
        "<strong>Key Questions to Consider:</strong>\n",
        "<ul>\n",
        "<li>Why do we use <code>Sigmoid</code> in the encoder bottleneck? What happens if we remove it?</li>\n",
        "<li>Why <code>Tanh</code> in the decoder output?</li>\n",
        "<li>What happens if we make the bottleneck even smaller (e.g., 2 neurons)?</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "## Experiment Setup\n",
        "\n",
        "We'll train our autoencoder with **5 latent dimensions** on MNIST digits. Each image is 28x28 pixels. This means we're compressing 784-dimensional data into just 5 numbers. \\\n",
        "We start from image data since it's easy to interpret and judge the reconstruction quality visually, but the very same applies to other data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMBFRdOttAdA"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 5\n",
        "NOISE_RATE = 0\n",
        "DATA_RANGE = (-1, 1)\n",
        "\n",
        "MODEL_NAME = 'ae_model'\n",
        "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "collate_fn = partial(collate_ae_dataset, noise_rate=NOISE_RATE, data_range=DATA_RANGE)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfsArKpntAdA"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_autoencoder_architecture(model, in_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Iteration on the dataloader\n",
        "# for batch_idx, (noisy_imgs, imgs, labels) in enumerate(valid_loader):\n",
        "#     print(\"Batch idx:\", batch_idx)\n",
        "#     # print(\"Noisy imgs:\", \"\\ttype:\", type(noisy_imgs), \"\\tlen:\", len(noisy_imgs), \"\\tshape:\", noisy_imgs.shape)\n",
        "#     print(\"Imgs:\", \"\\t\\ttype:\", type(imgs), \"\\tlen:\", len(imgs), \"\\tshape:\", imgs.shape)\n",
        "#     print(\"Labels:\", \"\\ttype:\", type(labels), \"\\tlen:\", len(labels), \"\\tshape:\", labels.shape)\n",
        "#     plt.hist(imgs.flatten().cpu().numpy(), bins=100, alpha=1);\n",
        "#     plt.hist(noisy_imgs.flatten().cpu().numpy(), bins=100, alpha=0.5);\n",
        "#     plt.yscale('log')\n",
        "#     plt.legend(['clean', 'noisy'])\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-lP40zxtAdC"
      },
      "source": [
        "Train the model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N1cGSNCtAdN"
      },
      "outputs": [],
      "source": [
        "# This will load a pre-trained model if available, otherwise start training\n",
        "\n",
        "N_EPOCHS = 50\n",
        "LR = 0.0009\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Check for existing trained model\n",
        "checkpoint_files = list(model_root.glob('model_*.pth'))\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "if checkpoint_files:\n",
        "    latest_checkpoint = max(checkpoint_files, key=lambda p: int(p.stem.split('_')[-1]))\n",
        "    \n",
        "    print(f\"Loading existing model from: {latest_checkpoint}\")\n",
        "    if device == torch.device('cpu'):\n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=torch.device('cpu'), weights_only=False)\n",
        "    else:\n",
        "        checkpoint = torch.load(latest_checkpoint, weights_only=False)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    \n",
        "    print(f\"Loaded model trained for {checkpoint['epoch'] + 1} epochs\")\n",
        "    \n",
        "    # Try to load training history if it exists\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    \n",
        "    if history_file.exists() and sample_history_file.exists():\n",
        "        import pickle\n",
        "        with open(history_file, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "        with open(sample_history_file, 'rb') as f:\n",
        "            sample_history = pickle.load(f)\n",
        "        print(f\"Loaded training history with {len(history['epoch'])} epochs\")\n",
        "    else:\n",
        "        print(\"No training history found, will start fresh history tracking\")\n",
        "        start_epoch = 0  # Reset if no history available\n",
        "    \n",
        "    if start_epoch >= N_EPOCHS:\n",
        "        print(f\"Model already trained for {start_epoch} epochs (target: {N_EPOCHS})\")\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} to {N_EPOCHS}\")\n",
        "        \n",
        "else:\n",
        "    # start training from scratch\n",
        "    print(f\"No existing model found - starting training from scratch for {N_EPOCHS} epochs\")\n",
        "    start_epoch = 0\n",
        "\n",
        "# Training loop\n",
        "if start_epoch < N_EPOCHS:\n",
        "    print(f\"Starting training...\")\n",
        "\n",
        "    pbar = tqdm.tqdm(range(start_epoch, N_EPOCHS), postfix=f'epoch {start_epoch}/{N_EPOCHS}')\n",
        "    for epoch_idx in pbar:\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(noisy_data)\n",
        "            loss_value = loss(output, data)\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss_value.detach().cpu().item()\n",
        "        epoch_loss /= len(train_loader)\n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['epoch'].append(epoch_idx)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "                output = model(noisy_data)\n",
        "                loss_value = loss(output, data)\n",
        "                val_loss += loss_value.detach().cpu().item()\n",
        "            val_loss /= len(valid_loader)\n",
        "            history['val_loss'].append(val_loss)\n",
        "\n",
        "        pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "        \n",
        "        # evaluate on samples\n",
        "        sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "        sample_history[epoch_idx] = sample_res\n",
        "\n",
        "        # save model weights\n",
        "        torch.save({\n",
        "                    'epoch': epoch_idx,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss\n",
        "                    }, model_root/f'model_{epoch_idx:03d}.pth')\n",
        "\n",
        "    # Save training history and sample history after training completes\n",
        "    print(\"Saving training history and sample history...\")\n",
        "    \n",
        "    import pickle\n",
        "    \n",
        "    # Save training history\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    with open(history_file, 'wb') as f:\n",
        "        pickle.dump(history, f)\n",
        "    \n",
        "    # Save sample history  \n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    with open(sample_history_file, 'wb') as f:\n",
        "        pickle.dump(sample_history, f)\n",
        "        \n",
        "    print(f\"Training history saved to: {history_file}\")\n",
        "    print(f\"Sample history saved to: {sample_history_file}\")\n",
        "    print(f\"Training completed - {len(history['epoch'])} epochs total\")\n",
        "\n",
        "else:\n",
        "    print(\"Model already fully trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3ZYMHrtAdO"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1F-fhnitAdP"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuIjJna0tAdU"
      },
      "source": [
        "Let's visually compare network's output with the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0HxF-QktAdU"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=10, fig_scale=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzL1wwdEHPfB"
      },
      "outputs": [],
      "source": [
        "sample_history[49]['z'][0].T.shape  # 320 samples (32 each class) x 5 elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co7xehyUGsCx"
      },
      "outputs": [],
      "source": [
        "for d in sample_history[49]['z'][0].T:\n",
        "  plt.hist(d, 100, alpha=0.3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_compression_metrics(model, samples, device):\n",
        "    \"\"\"\n",
        "    Compute comprehensive compression metrics for the autoencoder\n",
        "    \n",
        "    Args:\n",
        "        model: Trained autoencoder model\n",
        "        samples: Dictionary containing validation samples\n",
        "        device: PyTorch device (CPU/GPU)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with various compression metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Get sample data\n",
        "    original_images = torch.tensor(samples['images']).to(device)\n",
        "    labels = samples['labels']\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get latent representations\n",
        "        latent_codes = model.encode(original_images)\n",
        "        reconstructions = model.decode(latent_codes)\n",
        "    \n",
        "    # Convert to numpy for analysis\n",
        "    original_np = original_images.cpu().numpy()\n",
        "    latent_np = latent_codes.cpu().numpy()\n",
        "    reconstructed_np = reconstructions.cpu().numpy()\n",
        "    \n",
        "    # Dimensions\n",
        "    original_shape = original_np.shape[1:]  # (1, 28, 28)\n",
        "    latent_shape = latent_np.shape[1:]      # (code_size,)\n",
        "    \n",
        "    original_size = np.prod(original_shape)  # 784 pixels\n",
        "    latent_size = np.prod(latent_shape)      # code_size\n",
        "    \n",
        "    # Basic compression ratio\n",
        "    compression_ratio = original_size / latent_size\n",
        "    compression_percentage = (1 - latent_size/original_size) * 100\n",
        "    \n",
        "    # Assuming 32-bit floats (4 bytes per value)\n",
        "    bytes_per_value = 4\n",
        "    \n",
        "    original_bytes = original_size * bytes_per_value  # 784 * 4 = 3136 bytes\n",
        "    latent_bytes = latent_size * bytes_per_value      # code_size * 4 bytes\n",
        "    \n",
        "    size_compression_ratio = original_bytes / latent_bytes\n",
        "    size_reduction_percentage = (1 - latent_bytes/original_bytes) * 100\n",
        "    \n",
        "    # Entropy analysis (measure of information content)\n",
        "    def compute_entropy(data, bins=256):\n",
        "        \"\"\"Compute entropy of data\"\"\"\n",
        "        # Flatten and normalize to [0, 1]\n",
        "        flat_data = data.flatten()\n",
        "        normalized = (flat_data - flat_data.min()) / (flat_data.max() - flat_data.min() + 1e-8)\n",
        "        \n",
        "        # Create histogram\n",
        "        hist, _ = np.histogram(normalized, bins=bins, density=True)\n",
        "        hist = hist + 1e-12  # Avoid log(0)\n",
        "        hist = hist / hist.sum()  # Normalize\n",
        "        \n",
        "        # Compute entropy\n",
        "        entropy = -np.sum(hist * np.log2(hist))\n",
        "        return entropy\n",
        "    \n",
        "    original_entropy = compute_entropy(original_np)\n",
        "    latent_entropy = compute_entropy(latent_np)\n",
        "    reconstructed_entropy = compute_entropy(reconstructed_np)\n",
        "    \n",
        "    # Information retention ratio\n",
        "    info_retention = latent_entropy / original_entropy if original_entropy > 0 else 0\n",
        "    \n",
        "    # Latent space statistics\n",
        "    latent_mean = np.mean(latent_np, axis=0)\n",
        "    latent_std = np.std(latent_np, axis=0)\n",
        "    latent_range = np.ptp(latent_np, axis=0)  # peak-to-peak (max - min)\n",
        "    \n",
        "    # Effective dimensionality (based on PCA)\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA()\n",
        "    pca.fit(latent_np)\n",
        "    \n",
        "    # 95% variance threshold\n",
        "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
        "    effective_dims_95 = np.argmax(cumvar >= 0.95) + 1\n",
        "    effective_dims_99 = np.argmax(cumvar >= 0.99) + 1\n",
        "    \n",
        "    metrics = {\n",
        "        # Basic metrics\n",
        "        'original_dimensions': original_shape,\n",
        "        'latent_dimensions': latent_shape,\n",
        "        'original_size': original_size,\n",
        "        'latent_size': latent_size,\n",
        "        \n",
        "        # Compression ratios\n",
        "        'compression_ratio': compression_ratio,\n",
        "        'compression_percentage': compression_percentage,\n",
        "        'size_compression_ratio': size_compression_ratio,\n",
        "        'size_reduction_percentage': size_reduction_percentage,\n",
        "        \n",
        "        # Storage savings\n",
        "        'original_bytes_per_image': original_bytes,\n",
        "        'latent_bytes_per_image': latent_bytes,\n",
        "        'bytes_saved_per_image': original_bytes - latent_bytes,\n",
        "        \n",
        "        # Information content\n",
        "        'original_entropy': original_entropy,\n",
        "        'latent_entropy': latent_entropy,\n",
        "        'reconstructed_entropy': reconstructed_entropy,\n",
        "        'information_retention_ratio': info_retention,\n",
        "\n",
        "        # Latent space properties\n",
        "        'latent_mean': latent_mean,\n",
        "        'latent_std': latent_std,\n",
        "        'latent_range': latent_range,\n",
        "        'effective_dims_95': effective_dims_95,\n",
        "        'effective_dims_99': effective_dims_99,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        \n",
        "        # Per-class analysis\n",
        "        'labels': labels\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def print_compression_metrics(metrics):\n",
        "    print(f\"   Original Image Shape: {metrics['original_dimensions']}\")\n",
        "    print(f\"   Latent Vector Shape:  {metrics['latent_dimensions']}\")\n",
        "    print(f\"   Original Size:        {metrics['original_size']:,} values\")\n",
        "    print(f\"   Latent Size:          {metrics['latent_size']:,} values\")\n",
        "    print()\n",
        "    print(f\"   Dimensional Compression:  {metrics['compression_ratio']:.1f}:1\")\n",
        "    print(f\"   Space Reduction:          {metrics['compression_percentage']:.1f}%\")\n",
        "    print()\n",
        "    print(f\"   Original Size:        {metrics['original_bytes_per_image']:,} bytes per image\")\n",
        "    print(f\"   Compressed Size:      {metrics['latent_bytes_per_image']:,} bytes per image\")\n",
        "    print(f\"   For 10K images:       {metrics['bytes_saved_per_image']*10000/1024/1024:.1f} MB saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = compute_compression_metrics(model, samples, device)\n",
        "print_compression_metrics(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-YdzZ2QDwLg"
      },
      "source": [
        "# Denoising Autoencoder\n",
        "\n",
        "<div>\n",
        "\n",
        "<h3> The Denoising Challenge</h3>\n",
        "\n",
        "**Regular Autoencoder Problem:** Can simply learn the identity function (especially with large latent space)\n",
        "\n",
        "**Denoising Autoencoder Solution:** \n",
        "- Add noise to input images\n",
        "- Force the network to reconstruct the **clean** original\n",
        "- This prevents trivial solutions and encourages robust representations\n",
        "\n",
        "$$\\text{Noisy Input} \\xrightarrow{\\text{Encoder}} \\text{Latent} \\xrightarrow{\\text{Decoder}} \\text{Clean Output}$$\n",
        "\n",
        "<div>\n",
        "<strong>Noise Strategy:</strong> We'll corrupt 20% of pixels with salt-and-pepper noise\n",
        "<ul>\n",
        "<li><strong>Salt:</strong> Random pixels set to +1 (white)</li>\n",
        "<li><strong>Pepper:</strong> Random pixels set to -1 (black)</li>\n",
        "<li><strong>Effect:</strong> Forces the network to \"fill in\" missing information</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "**Benefits:**\n",
        "- More robust latent representations\n",
        "- Better generalization\n",
        "- Implicit regularization effect\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r45WCm8FtAdW"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 10\n",
        "NOISE_RATE = 0.2\n",
        "DATA_RANGE = (-1, 1)\n",
        "MODEL_NAME = 'dae_model'\n",
        "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "collate_fn = partial(collate_ae_dataset, noise_rate=NOISE_RATE, data_range=DATA_RANGE)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZgXG8vQtAdW"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_autoencoder_architecture(model, in_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N6i-gXjtAdX"
      },
      "source": [
        "Train the model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UKAEm9RtAdc"
      },
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 50\n",
        "LR = 0.0009\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# implement loss explicitly\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# train the model\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "# Check for existing trained model\n",
        "checkpoint_files = list(model_root.glob('model_*.pth'))\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "if checkpoint_files:\n",
        "    latest_checkpoint = max(checkpoint_files, key=lambda p: int(p.stem.split('_')[-1]))\n",
        "    \n",
        "    print(f\"Loading existing model from: {latest_checkpoint}\")\n",
        "    if device == torch.device('cpu'):\n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=torch.device('cpu'), weights_only=False)\n",
        "    else:\n",
        "        checkpoint = torch.load(latest_checkpoint, weights_only=False)\n",
        "    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    \n",
        "    print(f\"Loaded model trained for {checkpoint['epoch'] + 1} epochs\")\n",
        "    \n",
        "    # Try to load training history if it exists\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    \n",
        "    if history_file.exists() and sample_history_file.exists():\n",
        "        import pickle\n",
        "        with open(history_file, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "        with open(sample_history_file, 'rb') as f:\n",
        "            sample_history = pickle.load(f)\n",
        "        print(f\"Loaded training history with {len(history['epoch'])} epochs\")\n",
        "    else:\n",
        "        print(\"No training history found, will start fresh history tracking\")\n",
        "        start_epoch = 0  # Reset if no history available\n",
        "    \n",
        "    if start_epoch >= N_EPOCHS:\n",
        "        print(f\"Model already trained for {start_epoch} epochs (target: {N_EPOCHS})\")\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} to {N_EPOCHS}\")\n",
        "        \n",
        "else:\n",
        "    # start training from scratch\n",
        "    print(f\"No existing model found - starting training from scratch for {N_EPOCHS} epochs\")\n",
        "    start_epoch = 0\n",
        "\n",
        "# Training loop\n",
        "if start_epoch < N_EPOCHS:\n",
        "    print(f\"Starting training...\")\n",
        "\n",
        "    pbar = tqdm.tqdm(range(start_epoch, N_EPOCHS), postfix=f'epoch {start_epoch}/{N_EPOCHS}')\n",
        "    for epoch_idx in pbar:\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(noisy_data)\n",
        "            loss_value = loss(output, data)\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss_value.detach().cpu().item()\n",
        "        epoch_loss /= len(train_loader)\n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['epoch'].append(epoch_idx)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "                output = model(noisy_data)\n",
        "                loss_value = loss(output, data)\n",
        "                val_loss += loss_value.detach().cpu().item()\n",
        "            val_loss /= len(valid_loader)\n",
        "            history['val_loss'].append(val_loss)\n",
        "\n",
        "        pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "        \n",
        "        # evaluate on samples\n",
        "        sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "        sample_history[epoch_idx] = sample_res\n",
        "\n",
        "        # save model weights\n",
        "        torch.save({\n",
        "                    'epoch': epoch_idx,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss\n",
        "                    }, model_root/f'model_{epoch_idx:03d}.pth')\n",
        "\n",
        "    # Save training history and sample history after training completes\n",
        "    print(\"Saving training history and sample history...\")\n",
        "    \n",
        "    import pickle\n",
        "    \n",
        "    # Save training history\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    with open(history_file, 'wb') as f:\n",
        "        pickle.dump(history, f)\n",
        "    \n",
        "    # Save sample history  \n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    with open(sample_history_file, 'wb') as f:\n",
        "        pickle.dump(sample_history, f)\n",
        "        \n",
        "    print(f\"Training history saved to: {history_file}\")\n",
        "    print(f\"Sample history saved to: {sample_history_file}\")\n",
        "    print(f\"Training completed - {len(history['epoch'])} epochs total\")\n",
        "\n",
        "else:\n",
        "    print(\"Model already fully trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqyRCzactAdo"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgqjO5RPtAdp"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmD1KtcltAdr"
      },
      "source": [
        "Visualise evolution of reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs-PtmOAtAdr"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=10, fig_scale=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4saA2FcDFcMq"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "<div>\n",
        "\n",
        "## Exercise 1: Anomaly Detection with Autoencoders\n",
        "\n",
        "<div>\n",
        "<h4>Objective</h4>\n",
        "Use the trained autoencoder to identify outliers in the validation dataset by analyzing reconstruction errors.\n",
        "</div>\n",
        "\n",
        "**Your Task:**\n",
        "\n",
        "1. **Compute Reconstruction Errors**\n",
        "   - Run the trained model on the entire validation dataset\n",
        "   - Calculate loss for each sample\n",
        "   - Store both the loss values and the corresponding images\n",
        "\n",
        "2. **Analyze Error Distribution** \n",
        "   - Plot histogram of reconstruction errors\n",
        "   - Identify appropriate threshold for \"high error\" samples\n",
        "\n",
        "3. **Visualize Anomalies**\n",
        "   - Extract samples with highest reconstruction errors (top 1-5%)\n",
        "   - Display these \"poorly reconstructed\" samples\n",
        "\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "\n",
        "### Advanced Challenge (Homework)\n",
        "\n",
        "**UMAP Visualization of Latent Space**\n",
        "\n",
        "1. **Extract Features**:\n",
        "   - Get latent representations (z) for entire validation set\n",
        "   - Also get raw pixel values (flattened)\n",
        "\n",
        "3. **Create 2D Embeddings**:\n",
        "   - Apply UMAP to both raw data and latent representations\n",
        "   - Plot both embeddings side by side\n",
        "   - Color points by digit class\n",
        "\n",
        "4. **Highlight Outliers**:\n",
        "   - Mark anomalous samples (high reconstruction error) in red\n",
        "   - Compare their positions in both embedding spaces\n",
        "\n",
        "Are anomalies clustered or scattered in the embeddings?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu5h3HaT6aCN"
      },
      "source": [
        "# Convolutional Autoencoder {#convolutional-autoencoder}\n",
        "\n",
        "<div>\n",
        "\n",
        "<h3> Why Convolutional Layers?</h3>\n",
        "\n",
        "**Problems with Fully Connected Autoencoders:**\n",
        "- Ignore spatial structure of images\n",
        "- Too many parameters (784 → 128 = 100,352 weights!)\n",
        "- Poor translation invariance\n",
        "\n",
        "**Convolutional Autoencoder Advantages:**\n",
        "- **Spatial Awareness:** Preserves local image structure\n",
        "- **Parameter Sharing:** Dramatically fewer parameters\n",
        "- **Translation Invariance:** Robust to object positions\n",
        "- **Hierarchical Features:** Learns multi-scale representations\n",
        "\n",
        "<div>\n",
        "<h4>Architecture Design</h4>\n",
        "\n",
        "**Encoder (Downsampling Path):**\n",
        "```\n",
        "Input: 1×28×28\n",
        "Conv2d → 8×28×28 (features extraction)\n",
        "Conv2d → 8×14×14 (stride=2, downsampling)  \n",
        "Conv2d → 16×7×7 (stride=2, more features)\n",
        "Conv2d → 16×4×4 (stride=2, further compression)\n",
        "Conv2d → 32×2×2 (stride=2, final spatial compression)\n",
        "Flatten + Linear → CODE_SIZE (final compression)\n",
        "```\n",
        "\n",
        "**Decoder (Upsampling Path):**\n",
        "```\n",
        "Linear → 32×2×2 (unflatten)\n",
        "ConvTranspose2d → 16×4×4 (stride=2, upsample)\n",
        "ConvTranspose2d → 16×7×7 (stride=2, upsample)\n",
        "ConvTranspose2d → 8×14×14 (stride=2, upsample)\n",
        "ConvTranspose2d → 8×28×28 (stride=2, upsample)\n",
        "Conv2d → 1×28×28 (final reconstruction)\n",
        "```\n",
        "</div>\n",
        "\n",
        "**Key Insight:** We can now use much smaller latent spaces (even 2D) while maintaining good reconstruction quality.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf1wpycv6djD"
      },
      "source": [
        "Instead of fully connected layers we can use strided convolutional layers in encoder, and transposed convolutions in decoder.\n",
        "This model will have less parameters due to the weight sharing, thus easier to train.\n",
        "\n",
        "After upscaling the image size will be a bit bigger then original, so we also crop reconstruction to the input image size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTsTRZ-atAdu"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalAutoEncoder(AutoEncoder):\n",
        "    def __init__(self, input_size, code_size):\n",
        "        self.input_size = list(input_size)  # shape of data sample\n",
        "\n",
        "        self.hidden_size = 32*2*2\n",
        "\n",
        "        self.code_size = code_size  # code size\n",
        "\n",
        "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,   8, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(8,   8, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(8,  16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, self.hidden_size//8), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Linear(self.hidden_size//8, self.code_size),\n",
        "            # nn.Tanh(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.code_size, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Unflatten(1, (32, 2, 2)),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.ConvTranspose2d(16,  8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.ConvTranspose2d(8,   8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(8, 1, 3, padding=1, stride=1), nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def decode(self, z):\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
        "        return reconstruction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUZaXBP5tAdu"
      },
      "source": [
        "Thus we can try to reduce sise of the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySMlIeMdtAdv"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 2\n",
        "NOISE_RATE = 0.2\n",
        "MODEL_NAME = 'cdae_model'\n",
        "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "collate_fn = partial(collate_ae_dataset, noise_rate=NOISE_RATE, data_range=DATA_RANGE)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hkKRmFfyfZu"
      },
      "outputs": [],
      "source": [
        "xns = torch.tensor(samples['images_noisy']).to(device)\n",
        "print(xns.shape)\n",
        "zs = model.encode(xns)\n",
        "ys = model(xns)\n",
        "print(zs.shape)\n",
        "print(ys.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIexAmnYtAdw"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdqOdtc-tAdw"
      },
      "outputs": [],
      "source": [
        "model.get_n_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_autoencoder_architecture(model, in_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAqBhNcAtAdw"
      },
      "source": [
        "Train the model for 70 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBP1oJqKtAdx"
      },
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 70\n",
        "LR = 0.0004\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# implement loss explicitly\n",
        "loss = nn.L1Loss()\n",
        "\n",
        "# Check for existing trained model\n",
        "checkpoint_files = list(model_root.glob('model_*.pth'))\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "if checkpoint_files:\n",
        "    latest_checkpoint = max(checkpoint_files, key=lambda p: int(p.stem.split('_')[-1]))\n",
        "    \n",
        "    print(f\"Loading existing model from: {latest_checkpoint}\")\n",
        "    if device == torch.device('cpu'):\n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=torch.device('cpu'), weights_only=False)\n",
        "    else:\n",
        "        checkpoint = torch.load(latest_checkpoint, weights_only=False)\n",
        "    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    \n",
        "    print(f\"Loaded model trained for {checkpoint['epoch'] + 1} epochs\")\n",
        "    \n",
        "    # Try to load training history if it exists\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    \n",
        "    if history_file.exists() and sample_history_file.exists():\n",
        "        import pickle\n",
        "        with open(history_file, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "        with open(sample_history_file, 'rb') as f:\n",
        "            sample_history = pickle.load(f)\n",
        "        print(f\"Loaded training history with {len(history['epoch'])} epochs\")\n",
        "    else:\n",
        "        print(\"No training history found, will start fresh history tracking\")\n",
        "        start_epoch = 0  # Reset if no history available\n",
        "    \n",
        "    if start_epoch >= N_EPOCHS:\n",
        "        print(f\"Model already trained for {start_epoch} epochs (target: {N_EPOCHS})\")\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} to {N_EPOCHS}\")\n",
        "        \n",
        "else:\n",
        "    # start training from scratch\n",
        "    print(f\"No existing model found - starting training from scratch for {N_EPOCHS} epochs\")\n",
        "    start_epoch = 0\n",
        "\n",
        "# Training loop\n",
        "if start_epoch < N_EPOCHS:\n",
        "    print(f\"Starting training...\")\n",
        "\n",
        "    pbar = tqdm.tqdm(range(start_epoch, N_EPOCHS), postfix=f'epoch {start_epoch}/{N_EPOCHS}')\n",
        "    for epoch_idx in pbar:\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(noisy_data)\n",
        "            loss_value = loss(output, data)\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss_value.detach().cpu().item()\n",
        "        epoch_loss /= len(train_loader)\n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['epoch'].append(epoch_idx)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "                output = model(noisy_data)\n",
        "                loss_value = loss(output, data)\n",
        "                val_loss += loss_value.detach().cpu().item()\n",
        "            val_loss /= len(valid_loader)\n",
        "            history['val_loss'].append(val_loss)\n",
        "\n",
        "        pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "        \n",
        "        # evaluate on samples\n",
        "        sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "        sample_history[epoch_idx] = sample_res\n",
        "\n",
        "        # save model weights\n",
        "        torch.save({\n",
        "                    'epoch': epoch_idx,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss\n",
        "                    }, model_root/f'model_{epoch_idx:03d}.pth')\n",
        "\n",
        "    # Save training history and sample history after training completes\n",
        "    print(\"Saving training history and sample history...\")\n",
        "    \n",
        "    import pickle\n",
        "    \n",
        "    # Save training history\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    with open(history_file, 'wb') as f:\n",
        "        pickle.dump(history, f)\n",
        "    \n",
        "    # Save sample history  \n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    with open(sample_history_file, 'wb') as f:\n",
        "        pickle.dump(sample_history, f)\n",
        "        \n",
        "    print(f\"Training history saved to: {history_file}\")\n",
        "    print(f\"Sample history saved to: {sample_history_file}\")\n",
        "    print(f\"Training completed - {len(history['epoch'])} epochs total\")\n",
        "\n",
        "else:\n",
        "    print(\"Model already fully trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccaQpXXptAdy"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYXjiKE1tAdy"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSvnZCWvtAd0"
      },
      "source": [
        "Visualise evolution of reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRwLeOzGtAd0"
      },
      "outputs": [],
      "source": [
        "plt.hist(samples['images'].flatten(), bins=100, log=True);\n",
        "plt.show()\n",
        "plt.close()\n",
        "for k in ['y', 'z']:\n",
        "    # print(sample_history[0]['y'].shape)\n",
        "    plt.hist(np.array(sample_history[0][k]).flatten(), bins=100, log=True);\n",
        "    plt.hist(np.array(sample_history[N_EPOCHS-1][k]).flatten(), bins=100, log=True, alpha=0.3);\n",
        "    plt.legend(['first epoch', 'last epoch'])\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkLWD8yXtAd0"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=10, fig_scale=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Reconstruction Quality Analysis:\")\n",
        "visualize_reconstruction_quality_analysis(model, samples, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVUXN5Jq7kx9"
      },
      "source": [
        "## Latent space\n",
        "\n",
        "So far we looked just on the output: the model sort of does the job. But what does it learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mVBP8b0t-Wl"
      },
      "source": [
        "First let's use animation to visualize reconstruction evolution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wzDeCVObTlI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "single_el_idx = samples['single_el_idx']\n",
        "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
        "images = samples['images'][single_el_idx, 0]\n",
        "\n",
        "smpl_ims = []\n",
        "for epoch_idx, hist_el in sample_history.items():\n",
        "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
        "    smpl_ims.append(samples_arr)\n",
        "\n",
        "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
        "\n",
        "s=1\n",
        "fig = plt.figure(figsize=(s*nx, s*ny))\n",
        "\n",
        "m = mosaic(smpl_ims[0])\n",
        "\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "# plot 0th epoch - 0th frame\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=-0.5, vmax=0.5)\n",
        "\n",
        "# this function will be called to render each of the frames\n",
        "def animate(i):\n",
        "    m = mosaic(smpl_ims[i])\n",
        "    imsh.set_data(m)\n",
        "\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "\n",
        "    return imsh\n",
        "\n",
        "# create animation\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p25DtqevVw7q"
      },
      "outputs": [],
      "source": [
        "# display animation\n",
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qu6J9kGuEt9"
      },
      "source": [
        "And let's see evolution of the latent representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngMUudwdDpg5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "labels = samples['labels']\n",
        "epochs = sorted(sample_history.keys())\n",
        "z_res = [sample_history[ep]['z'][0] for ep in epochs]\n",
        "\n",
        "\n",
        "scat = plt.scatter(z_res[0][:,0], z_res[0][:,1], c=labels, cmap=cm.rainbow)\n",
        "plt.xlim(-6.1, 6.1)\n",
        "plt.ylim(-6.1, 6.1)\n",
        "\n",
        "ax = plt.gca()\n",
        "legend1 = ax.legend(*scat.legend_elements(), title=\"digits\")\n",
        "ax.add_artist(legend1)\n",
        "ax.set_aspect('equal')\n",
        "ttl = plt.title(f'after epoch {0}')\n",
        "\n",
        "def animate(i):\n",
        "    z = z_res[i]\n",
        "    scat.set_offsets(z)\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "    return scat\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-sYawGtV1le"
      },
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qArUdx6YHzD"
      },
      "source": [
        "## Sampling from latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syOBR27sn6vm"
      },
      "source": [
        "But we can also use the trained model to generate samples based on the latent representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLRUV7ad5P8p"
      },
      "outputs": [],
      "source": [
        "z0 = np.random.uniform(size=(25, CODE_SIZE))*12-6  # get 20 random points in 2D sampled from uniform distribution between 0 and 1\n",
        "z0_t = torch.tensor(z0, dtype=torch.float32).to(device)\n",
        "\n",
        "ims_all = []\n",
        "\n",
        "# this function will be called in saved model state after each training epoch\n",
        "def fn(ae):\n",
        "  with torch.no_grad():\n",
        "    ims = ae.decode(z0_t)\n",
        "    ims = ims.detach().cpu().numpy()\n",
        "    ims_all.append(ims)\n",
        "\n",
        "run_on_all_training_history(model, model_root, fn, device)\n",
        "\n",
        "ims_all = np.array(ims_all)\n",
        "print(ims_all.shape)\n",
        "ims_all = ims_all[:, :, 0, :, :]  # remove channel dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdGfJcLItAeC"
      },
      "outputs": [],
      "source": [
        "plt.hist(ims_all[0].flatten(), bins=100, log=True);\n",
        "plt.hist(ims_all[-1].flatten(), bins=100, log=True, alpha=0.5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edg10WV74629"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "fig = plt.figure(figsize=(20, 1.5))\n",
        "\n",
        "m = mosaic([ims_all[0]])\n",
        "\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    m = mosaic([ims_all[i]])\n",
        "    imsh.set_data(m)\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "    return imsh\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(ims_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy49wsQJV5Ky"
      },
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk1sA3cNFuLB"
      },
      "source": [
        "## Interpolation in latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsyVm5m_F3ng"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "zs = np.meshgrid(np.linspace(-6, 6, n_samples),\n",
        "                 np.linspace(-6, 6, n_samples))\n",
        "zs = np.stack(zs, axis=-1).reshape(-1, 2)\n",
        "zs_t = torch.tensor(zs, dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Latent Space Exploration & Analysis\n",
        "\n",
        "print(\"Interactive Latent Space Grid Exploration:\")\n",
        "print(\"This shows what the decoder generates at different points in latent space\")\n",
        "visualize_latent_interpolation_grid(model, device, latent_dim=CODE_SIZE, n_steps=8, range_vals=(-4, 4))\n",
        "\n",
        "print(\"Digit-to-Digit Interpolation:\")\n",
        "print(\"Watch how one digit morphs into another by interpolating in latent space\")\n",
        "visualize_digit_interpolation(model, samples, device, digit1=3, digit2=8, n_steps=10)\n",
        "visualize_digit_interpolation(model, samples, device, digit1=1, digit2=7, n_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Latent Space Explorer\n",
        "\n",
        "<div>\n",
        "<h3>Hands-on Exploration</h3>\n",
        "<p style=\"margin: 0;\">Use the interactive widgets below to explore what your trained autoencoder has learned!</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Latent Space Explorer\n",
        "# Note: This will only work if you have a trained model with at least 2 latent dimensions\n",
        "\n",
        "try:\n",
        "    if CODE_SIZE >= 2:\n",
        "        explorer = create_interactive_latent_explorer(model, samples, device)\n",
        "        display(explorer)\n",
        "    else:\n",
        "        print(\"Interactive explorer requires at least 2 latent dimensions!\")\n",
        "        print(f\"Current model has {CODE_SIZE} dimensions.\")\n",
        "except NameError:\n",
        "    print(\"Please train a convolutional autoencoder first!\")\n",
        "except Exception as e:\n",
        "    print(f\"Interactive widgets not available: {e}\")\n",
        "    print(\"You can still explore latent space using the grid sampling approach above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Noise Level Demo\n",
        "# Explore how different noise levels affect the input images\n",
        "\n",
        "try:\n",
        "    noise_widget = create_noise_comparison_widget()\n",
        "    display(noise_widget)\n",
        "except Exception as e:\n",
        "    print(f\"Interactive widgets not available: {e}\")\n",
        "    print(\"You can still manually compare noise levels using the NOISE_RATE parameter!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 2: Hyperparameter Exploration\n",
        "\n",
        "<div>\n",
        "\n",
        "**Work in 2 groups**\n",
        "\n",
        "<div>\n",
        "\n",
        "### Option 1: Noise Robustness Study\n",
        "How does noise level affect learned representations?\n",
        "\n",
        "1. Train 4 autoencoders with noise rates: `[0.0, 0.2, 0.6, 0.8]`\n",
        "2. Keep all other hyperparameters constant (architecture, epochs, learning rate)\n",
        "3. For each model, analyze:\n",
        "   - **Reconstruction Quality**: Visual comparison + MSE\n",
        "   - **Latent Space Structure**: 2D scatter plots colored by digit\n",
        "   - **Training Dynamics**: Loss curves comparison\n",
        "\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "\n",
        "### Option 2: Latent Dimensionality Study  \n",
        "How does bottleneck size affect compression vs. quality trade-off?\n",
        "\n",
        "1. Train 5 autoencoders with latent sizes: `[2, 4, 8, 16, 32]`\n",
        "2. Use fixed noise rate (e.g., 0.2) and other hyperparameters\n",
        "3. For each model, analyze:\n",
        "   - **Compression Ratio**: Input size / Latent size\n",
        "   - **Reconstruction Quality**: Visual comparison + MSE\n",
        "   - **Parameter Count**: Model size comparison\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBT5XbLztAeH"
      },
      "source": [
        "## Exercise 3: Writing latent Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A85koGGYtAeK"
      },
      "source": [
        "\n",
        "In this exercise, we will\n",
        "1. modify the CNN to be fully convolutional, with 4 downscaling layers (x16), 16 channels\n",
        "2. save dataset of latent states, see code below for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iS4ZCW4tAeK"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_pckl(obj, fname):\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "def load_pckl(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPCKd8__tAeM"
      },
      "outputs": [],
      "source": [
        "#load latest model checkpoint:\n",
        "run_on_trained(model, model_root, lambda m:None, device, ep=3)\n",
        "\n",
        "# dataset for latent DDM, file list:\n",
        "data_files = {\"train\": [], \"valid\": []}\n",
        "\n",
        "ds_root = pl.Path('data')\n",
        "ds_root.mkdir(exist_ok=True)\n",
        "\n",
        "# generate latent values per blocks of samples:\n",
        "block_sz = 5000\n",
        "for ds, sfx in zip([valid_dataset, train_dataset], ['valid', 'train']):\n",
        "  x = ds.data.numpy().reshape(-1, 1, 28, 28)\n",
        "  x = x/255.*2-1\n",
        "  l = ds.targets.numpy()\n",
        "\n",
        "  for i in range(0, len(x), block_sz):\n",
        "    x_b = x[i:i+block_sz]\n",
        "    l_b = l[i:i+block_sz]\n",
        "\n",
        "    x_b_t = torch.tensor(x_b, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z_t = model.encode(x_b_t)\n",
        "        z_b = z_t.detach().cpu().numpy()\n",
        "\n",
        "    # save latent values and their shape and labels to a dataframe:\n",
        "    data_d = {\n",
        "    'z': [zi.flatten() for zi in z_b],\n",
        "    'shape': [zi.shape for zi in z_b],\n",
        "    'label': l_b\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data=data_d)\n",
        "\n",
        "    # save dataframe to a file and reference dictioanry:\n",
        "    fname = ds_root/f'df_z_{sfx}_{i}.pckl'\n",
        "    save_pckl(df, fname)\n",
        "    data_files[sfx].append(str(fname))\n",
        "\n",
        "save_pckl(data_files, ds_root/'data_files.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBIJwlWRtAeN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJELUTmKtAeO"
      },
      "outputs": [],
      "source": [
        "# now we can load the dataset:\n",
        "data_files = load_pckl(ds_root/'data_files.pkl')\n",
        "lds = load_dataset('pandas', data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BprdQfcEtAeU"
      },
      "outputs": [],
      "source": [
        "# convert to torch dataset:\n",
        "tds = lds.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tds['train']['z'][0].shape, tds['train']['label'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Key Takeaways & Summary\n",
        "\n",
        "<div style=\"padding: 25px; border-radius: 15px; margin: 20px 0;\">\n",
        "\n",
        "<h2 style=\"margin-top: 0;\"> What We've Learned</h2>\n",
        "\n",
        "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;\">\n",
        "\n",
        "<div style=\"padding: 15px; border-radius: 10px;\">\n",
        "<h4 style=\"margin-top: 0;\">Core Concepts</h4>\n",
        "<ul style=\"margin: 0; padding-left: 20px;\">\n",
        "<li><strong>Encoder-Decoder Architecture</strong></li>\n",
        "<li><strong>Latent Space Representations</strong></li>\n",
        "<li><strong>Dimensionality Reduction</strong></li>\n",
        "<li><strong>Reconstruction Loss</strong></li>\n",
        "<li><strong>Regularization through Noise</strong></li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<div style=\"padding: 15px; border-radius: 10px;\">\n",
        "<h4 style=\"margin-top: 0;\"> Practical Skills</h4>\n",
        "<ul style=\"margin: 0; padding-left: 20px;\">\n",
        "<li><strong>PyTorch Implementation</strong></li>\n",
        "<li><strong>Training Loop Design</strong></li>\n",
        "<li><strong>Visualization Techniques</strong></li>\n",
        "<li><strong>Hyperparameter Tuning</strong></li>\n",
        "<li><strong>Model Evaluation</strong></li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "<div style=\"margin: 20px 0;\">\n",
        "\n",
        "## Applications & Extensions\n",
        "\n",
        "**Where can you use autoencoders?**\n",
        "\n",
        "### **Creative Applications**\n",
        "- **Style Transfer**: Learn artistic style representations\n",
        "- **Image Colorization**: Convert grayscale to color images  \n",
        "- **Super-Resolution**: Enhance image quality\n",
        "- **Data Augmentation**: Generate training variations\n",
        "\n",
        "### **Analysis & Detection**\n",
        "- **Anomaly Detection**: Identify unusual patterns in data\n",
        "- **Fraud Detection**: Spot irregular financial transactions\n",
        "- **Medical Imaging**: Detect abnormalities in scans\n",
        "- **Quality Control**: Find defects in manufacturing\n",
        "\n",
        "### **Advanced Architectures**\n",
        "- **Variational Autoencoders (VAEs)**: Probabilistic latent spaces\n",
        "- **Beta-VAEs**: Disentangled representations\n",
        "- **Adversarial Autoencoders**: GAN-like training\n",
        "- **Transformer Autoencoders**: For sequential data\n",
        "\n",
        "</div>\n",
        "\n",
        "<div style=\"padding: 20px; border-radius: 15px; border-left:margin: 20px 0;\">\n",
        "\n",
        "## Design Principles We've Discovered\n",
        "\n",
        "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n",
        "\n",
        "<div>\n",
        "<h4> Architecture Choices</h4>\n",
        "<ul>\n",
        "<li><strong>Bottleneck Size</strong>: Balance compression vs. quality</li>\n",
        "<li><strong>Layer Depths</strong>: Deeper = more complex features</li>\n",
        "<li><strong>Activation Functions</strong>: Sigmoid/Tanh for bounded outputs</li>\n",
        "<li><strong>Convolutional Layers</strong>: Preserve spatial relationships</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<h4>Training Strategies</h4>\n",
        "<ul>\n",
        "<li><strong>Noise Injection</strong>: Prevents overfitting, improves robustness</li>\n",
        "<li><strong>Learning Rate</strong>: Start high, decay over time</li>\n",
        "<li><strong>Batch Size</strong>: Balance memory and gradient stability</li>\n",
        "<li><strong>Regularization</strong>: Dropout, weight decay, early stopping</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "<div style=\"padding: 20px; border-radius: 15px; border-left: margin: 20px 0;\">\n",
        "\n",
        "## Common Pitfalls & Solutions\n",
        "\n",
        "| Problem | Symptom | Solution |\n",
        "|---------|---------|----------|\n",
        "| **Identity Learning** | Perfect reconstruction, no compression | Add noise, reduce latent size |\n",
        "| **Mode Collapse** | All outputs look similar | Increase latent size, better initialization |\n",
        "| **Blurry Outputs** | Reconstructions lack sharp details | Use L1 loss, perceptual losses |\n",
        "| **Overfitting** | Training loss << validation loss | More data, regularization, early stopping |\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the previous model learn to seaparate subpopulations in the latent space, there remains significant overlap & unpopulated regions.\n",
        "\n",
        "Varitional AE puts additional constraints on the distribution in the latent space and perform variational inference.\n",
        "\n",
        "(see pptx for details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the encoder will predict `2 x n_code` values: means and logarithm of variance for each sample. Since these values live in $R^2$ - no activation function is used in last layer of the encoder.\n",
        "\n",
        "Then for reconstruction we will sample from this distribuition with a reparametrisation trick.\n",
        "\n",
        "The tecnically complex part - is to implement the custom loss function and training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VariationalConvolutionalAutoencoder(AutoEncoder):\n",
        "    def __init__(self, input_size, code_size):\n",
        "        super(VariationalConvolutionalAutoencoder, self).__init__(input_size, code_size)\n",
        "\n",
        "        # nn.LeakyReLU(negative_slope=0.3)\n",
        "        self.input_size = list(input_size)  # shape of data sample\n",
        "        self.npix = np.prod(self.input_size)\n",
        "\n",
        "        self.hidden_size = 64*1\n",
        "\n",
        "        self.code_size = code_size  # code size\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,   16, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16,  16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(32,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(32,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(32,  64, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, 64), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            # we want values in all R, thus no activation function is applied. self.n_code values for mean + self.n_code for log(variance)\n",
        "            nn.Linear(64, self.code_size * 2),\n",
        "\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.code_size, 64), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Linear(64, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Unflatten(1, (64, 1, 1)),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(16, 1, 3, padding=1, stride=1), nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            # samle from standard normal distribution\n",
        "            eps = torch.randn((100, self.code_size))\n",
        "        return self.decode(eps)\n",
        "\n",
        "    def encode(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z_mean, z_logvar = torch.split(z, split_size_or_sections=2, dim=1)\n",
        "        return z_mean, z_logvar\n",
        "\n",
        "    def reparameterize(self, z_mean, z_logvar):\n",
        "        # reaparametrization trick: to sample z from N(mean, std):\n",
        "        # z = mean + std * eps, where eps sampled from N(0, 1)\n",
        "        eps = torch.randn_like(z_mean)\n",
        "        z_std = torch.exp(z_logvar * .5)\n",
        "        return eps * z_std + z_mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x, return_z=False):\n",
        "        z_mean, z_logvar = self.encode(x)\n",
        "        z = self.reparameterize(z_mean, z_logvar)\n",
        "        reconstruction = self.decode(z)\n",
        "        return (reconstruction, z_mean, z_logvar) if return_z else reconstruction\n",
        "\n",
        "    def forward_and_KL_loss(self, x, y):\n",
        "        reconstruction, z_mean, z_logvar = self(x, return_z=True)\n",
        "\n",
        "        # tf impl:\n",
        "        # loss_z_kl = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + tf.square(z_mean) -1.0 - z_logvar, axis=1)  # KL divergence from N(0, 1) to N(z_mean, tf.exp(z_logvar * .5))\n",
        "        # loss_z_kl = tf.reduce_mean(loss_z_kl) / np.prod(self.data_size)  # but since we used mean in reconstruction loss - this term has to be normalized accordingly\n",
        "\n",
        "        # pytorch impl:\n",
        "        loss_z_kl = 0.5 * torch.sum(torch.exp(z_logvar) + torch.square(z_mean) -1.0 - z_logvar, dim=1)\n",
        "        loss_z_kl = torch.mean(loss_z_kl) / self.npix\n",
        "\n",
        "        return reconstruction, loss_z_kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CODE_SIZE = 2 # 50\n",
        "NOISE_RATE = 0.\n",
        "DATA_RANGE = (-1, 1)\n",
        "MODEL_NAME = 'vcae_model'\n",
        "model = VariationalConvolutionalAutoencoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "collate_fn = partial(collate_ae_dataset, noise_rate=NOISE_RATE, data_range=DATA_RANGE)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xns = torch.tensor(samples['images_noisy']).to(device)\n",
        "print(xns.shape)\n",
        "ys = model(xns)\n",
        "ys.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.get_n_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 20 # 20\n",
        "LR = 0.0009\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Check for existing trained model\n",
        "checkpoint_files = list(model_root.glob('model_*.pth'))\n",
        "history = {'loss': [], 'val_loss': [], 'rec_loss': [], 'rec_val_loss': [], 'kl_loss': [], 'kl_val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "if checkpoint_files:\n",
        "    latest_checkpoint = max(checkpoint_files, key=lambda p: int(p.stem.split('_')[-1]))\n",
        "    \n",
        "    print(f\"Loading existing model from: {latest_checkpoint}\")\n",
        "    if device == torch.device('cpu'):\n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=torch.device('cpu'), weights_only=False)\n",
        "    else:\n",
        "        checkpoint = torch.load(latest_checkpoint, weights_only=False)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    \n",
        "    print(f\"Loaded model trained for {checkpoint['epoch'] + 1} epochs\")\n",
        "    \n",
        "    # Try to load training history if it exists\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    \n",
        "    if history_file.exists() and sample_history_file.exists():\n",
        "        import pickle\n",
        "        with open(history_file, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "        with open(sample_history_file, 'rb') as f:\n",
        "            sample_history = pickle.load(f)\n",
        "        print(f\"Loaded training history with {len(history['epoch'])} epochs\")\n",
        "    else:\n",
        "        print(\"No training history found, will start fresh history tracking\")\n",
        "        start_epoch = 0  # Reset if no history available\n",
        "    \n",
        "    if start_epoch >= N_EPOCHS:\n",
        "        print(f\"Model already trained for {start_epoch} epochs (target: {N_EPOCHS})\")\n",
        "    else:\n",
        "        print(f\"Resuming training from epoch {start_epoch} to {N_EPOCHS}\")\n",
        "        \n",
        "else:\n",
        "    # start training from scratch\n",
        "    print(f\"No existing model found - starting training from scratch for {N_EPOCHS} epochs\")\n",
        "    start_epoch = 0\n",
        "\n",
        "# Training loop\n",
        "if start_epoch < N_EPOCHS:\n",
        "    print(f\"Starting training...\")\n",
        "    pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
        "    for epoch_idx in pbar:\n",
        "        epoch_loss = 0\n",
        "        epoch_rec_loss = 0\n",
        "        epoch_kl_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            #output = model(noisy_data)\n",
        "            output, kl_loss = model.forward_and_KL_loss(noisy_data, data)\n",
        "            rec_loss = loss(output, data)\n",
        "            loss_value = rec_loss + kl_loss\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss_value.detach().cpu().item()\n",
        "            epoch_rec_loss += rec_loss.detach().cpu().item()\n",
        "            epoch_kl_loss += kl_loss.detach().cpu().item()\n",
        "\n",
        "        n_elements = len(train_loader)\n",
        "\n",
        "        epoch_loss /= n_elements\n",
        "        epoch_rec_loss /= n_elements\n",
        "        epoch_kl_loss /= n_elements\n",
        "\n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['rec_loss'].append(epoch_rec_loss)\n",
        "        history['kl_loss'].append(epoch_kl_loss)\n",
        "\n",
        "        history['epoch'].append(epoch_idx)\n",
        "        # update progress bar\n",
        "\n",
        "        # evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            val_rec_loss = 0\n",
        "            val_kl_loss = 0\n",
        "\n",
        "            for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "                #output = model(noisy_data)\n",
        "                output, kl_loss = model.forward_and_KL_loss(noisy_data, data)\n",
        "                rec_loss = loss(output, data)\n",
        "                loss_value = rec_loss + kl_loss\n",
        "\n",
        "                val_loss += loss_value.detach().cpu().item()\n",
        "                val_rec_loss += rec_loss.detach().cpu().item()\n",
        "                val_kl_loss += kl_loss.detach().cpu().item()\n",
        "\n",
        "            val_loss /= len(valid_loader)\n",
        "            val_rec_loss /= len(valid_loader)\n",
        "            val_kl_loss /= len(valid_loader)\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['rec_val_loss'].append(val_rec_loss)\n",
        "            history['kl_val_loss'].append(val_kl_loss)\n",
        "\n",
        "        pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "        # evaluate on samples\n",
        "        sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "        sample_history[epoch_idx] = sample_res\n",
        "\n",
        "        # save model weights\n",
        "        torch.save({\n",
        "                    'epoch': epoch_idx,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss\n",
        "                    }, model_root/f'model_{epoch_idx:03d}.pth')\n",
        "\n",
        "    # Save training history and sample history after training completes\n",
        "    print(\"Saving training history and sample history...\")\n",
        "    \n",
        "    import pickle\n",
        "    \n",
        "    # Save training history\n",
        "    history_file = model_root / 'training_history.pkl'\n",
        "    with open(history_file, 'wb') as f:\n",
        "        pickle.dump(history, f)\n",
        "    \n",
        "    # Save sample history  \n",
        "    sample_history_file = model_root / 'sample_history.pkl'\n",
        "    with open(sample_history_file, 'wb') as f:\n",
        "        pickle.dump(sample_history, f)\n",
        "        \n",
        "    print(f\"Training history saved to: {history_file}\")\n",
        "    print(f\"Sample history saved to: {sample_history_file}\")\n",
        "    print(f\"Training completed - {len(history['epoch'])} epochs total\")\n",
        "\n",
        "else:\n",
        "    print(\"Model already fully trained.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize reconstruciton:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "single_el_idx = samples['single_el_idx']\n",
        "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
        "images = samples['images'][single_el_idx, 0]\n",
        "\n",
        "smpl_ims = []\n",
        "for epoch_idx, hist_el in sample_history.items():\n",
        "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
        "    smpl_ims.append(samples_arr)\n",
        "\n",
        "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
        "\n",
        "s=1\n",
        "fig = plt.figure(figsize=(s*nx, s*ny))\n",
        "\n",
        "m = mosaic(smpl_ims[0])\n",
        "\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "# plot 0th epoch - 0th frame\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "# this function will be called to render each of the frames\n",
        "def animate(i):\n",
        "    m = mosaic(smpl_ims[i])\n",
        "    imsh.set_data(m)\n",
        "\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "\n",
        "    return imsh\n",
        "\n",
        "# create animation\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the latent representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_history[0]['z'][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "smpl_epochs = list(sample_history.keys())\n",
        "\n",
        "z_res_mean = [sample_history[ep]['z'][0] for ep in smpl_epochs]\n",
        "z_logvar = [sample_history[ep]['z'][1] for ep in smpl_epochs]\n",
        "z_res_std = np.exp(np.array(z_logvar) * .5)\n",
        "\n",
        "\n",
        "l_res = samples['labels']\n",
        "l_col = [cm.rainbow(l_i/10) for l_i in l_res]\n",
        "\n",
        "# error bars:\n",
        "def get_lines(m, s):\n",
        "  l = []\n",
        "  for (x, y), (sx, sy) in zip(m, s):\n",
        "    l.append([(x-sx, y), (x+sx, y)]) # h\n",
        "    l.append([(x, y-sy), (x, y+sy)]) # w\n",
        "  return np.array(l)\n",
        "\n",
        "lines = get_lines(z_res_mean[0], z_res_std[0])\n",
        "lc = mc.LineCollection(lines, color=l_col, linewidths=2, alpha=0.3)\n",
        "fig.gca().add_collection(lc)\n",
        "\n",
        "scat = plt.scatter(z_res_mean[0][:,0], z_res_mean[0][:,1], c=l_res, cmap=cm.rainbow)\n",
        "#scat_err = plt.errorbar(z_res_mean[0][:,0], z_res_mean[0][:,1], xerr=z_res_std[0][:,0], yerr=z_res_std[0][:,1], fmt=\"o\")\n",
        "\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-4, 4)\n",
        "\n",
        "legend1 = plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
        "plt.gca().add_artist(legend1)\n",
        "plt.gca().set_aspect('equal')\n",
        "ttl = plt.title(f'after epoch {0}')\n",
        "\n",
        "def animate(i):\n",
        "    z = z_res_mean[i]\n",
        "    scat.set_offsets(z)\n",
        "\n",
        "    lines = get_lines(z_res_mean[i], z_res_std[i])\n",
        "    lc.set_segments(lines)\n",
        "\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "    return scat, lc\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(z_res_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And let's sample from the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 20\n",
        "zs = np.meshgrid(norm.ppf(np.linspace(0.0001, 0.9999, n)),  # sampling proportionally to the density\n",
        "                 norm.ppf(np.linspace(0.0001, 0.9999, n)))\n",
        "# zs = np.meshgrid(np.linspace(-4, 4, n),  # sampling linearly\n",
        "#                  np.linspace(-4, 4, n))\n",
        "zs = np.stack(zs, axis=-1)\n",
        "zs = zs.reshape((-1, 2))\n",
        "\n",
        "zs_t = torch.tensor(zs, dtype=torch.float32).to(device)\n",
        "\n",
        "def fn(ae):\n",
        "  with torch.no_grad():\n",
        "    ims_t = ae.decode(zs_t)\n",
        "    ims = ims_t.detach().cpu().numpy()\n",
        "    ims = ims[:, 0, :, :]  # remove channel dimension\n",
        "\n",
        "  sh = list(ims.shape)\n",
        "  ims = ims.reshape([n, n]+sh[1:])\n",
        "  plt.figure(figsize=(n, n))\n",
        "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "run_on_trained(model, model_root, fn, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fn(ae):\n",
        "  # x = samples['images_noisy']\n",
        "  # l = samples['labels']\n",
        "  # ds = valid_dataset\n",
        "  ds = train_dataset\n",
        "  x = ds.data.numpy().reshape(-1, 1, 28, 28)\n",
        "  x = x/255.*2-1\n",
        "  l = ds.targets.numpy()\n",
        "  x_t = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "  z_m, z_s = ae.encode(x_t)\n",
        "  z_m, z_s = [el.detach().cpu().numpy() for el in [z_m, z_s]]\n",
        "\n",
        "  z_res_std = np.exp(np.array(z_s) * .5)\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  scat = plt.scatter(*z_m.T, c=l, s=2, cmap='jet')\n",
        "  plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 2), ncols=2, nrows=1)\n",
        "  ax[0].hist(z_m.flatten(), bins=100);\n",
        "  ax[1].hist(z_res_std.flatten(), bins=100);\n",
        "  ax[0].set_title('mean')\n",
        "  ax[1].set_title('std')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "run_on_trained(model, model_root, fn, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization excercise (homework)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Option 1: Sample 31x31 and overlay with data points.\n",
        "\n",
        "Option 2: Visulize data-point in latent space\n",
        "\n",
        "\n",
        "```\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = plt.gca()\n",
        "\n",
        "# given the image sample array `image_arr` and the pair of coordinate arrays\n",
        "# of the latent representation z0_arr, z1_arr:\n",
        "for z0_i, z1_i, im_i in zip(z0_arr, z1_arr, image_arr):\n",
        "    im = OffsetImage(im_i, zoom=0.5)\n",
        "    ab = AnnotationBbox(im, (z0_i, z1_i), xycoords='data', frameon=False)\n",
        "\n",
        "    ax.add_artist(ab)\n",
        "    ax.update_datalim([(z0_i, z1_i)])\n",
        "    ax.autoscale()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example of project work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use VAE to explore Fashion MNIST, or any other dataset\n",
        " 1. train model\n",
        " 2. explore latent representation\n",
        " 3. find outliers\n",
        " 4. sample from the latent distribution\n",
        " 5. overlay with data points\n",
        " 6. Train classifier/regressor using the latent space representation of the samples"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "smjlo1x48N3t",
        "TVG7kpjf8T2D",
        "cbR9emfD8d41",
        "9QH3J3Z-jTb2",
        "7-YdzZ2QDwLg",
        "4saA2FcDFcMq",
        "tu5h3HaT6aCN",
        "AVUXN5Jq7kx9",
        "9qArUdx6YHzD",
        "Tk1sA3cNFuLB",
        "phTJlLGx3a5H",
        "8_tWAYpah3aG",
        "9PLAxq3XIho_",
        "lgVvSoXa5vOk",
        "G3DrSyps5wWv",
        "kHPnzzd_6PLO",
        "YVruUa0d7fMw",
        "G1_R6CnAy7-F"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "general",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
