{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD2PIYyZz23I"
      },
      "source": [
        "# Denoising Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyHF9zJKzzb7"
      },
      "source": [
        "<p>\n",
        "CAS on Advanced Machine Learning <br>\n",
        "Data Science Lab, University of Bern, 2024<br>\n",
        "Prepared by Dr. Mykhailo Vladymyrov.\n",
        "\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
        "Based on the HuggingFace tutorials and reference manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiWNh7VZz71D"
      },
      "source": [
        "# Libs installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdq1K-ho0362"
      },
      "outputs": [],
      "source": [
        "pip install transformers diffusers accelerate einops datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgWXQ4B-0Vs2"
      },
      "source": [
        "# Load libs and utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhtCXQSF_9eJ"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import einops as eo\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from diffusers import UNet2DModel\n",
        "from diffusers import DDPMScheduler\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from diffusers import DDPMPipeline\n",
        "from diffusers.utils import make_image_grid\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQvQtKv80Ssq"
      },
      "outputs": [],
      "source": [
        "def is_iterable(obj):\n",
        "    if type(obj) == str:\n",
        "      return False\n",
        "    try:\n",
        "        iter(obj)\n",
        "    except Exception:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def plot_many(ys, xs=None, labels=None, xlabels=None, ylabels=None, titles=None, legend_loc='best', single_plot_sz=6):\n",
        "    \"\"\"\n",
        "    plot many lines in one plot\n",
        "    \"\"\"\n",
        "    n_plots = len(ys)\n",
        "\n",
        "    def prep_for_n_plots(var, n_plots):\n",
        "        if var is None:\n",
        "            return [None] * n_plots\n",
        "        elif is_iterable(var):\n",
        "            assert len(var) == n_plots, f'len({var}) != {n_plots}'\n",
        "            return var\n",
        "        else:\n",
        "            return [var] * n_plots\n",
        "\n",
        "    xs = prep_for_n_plots(xs, n_plots)\n",
        "    labels = prep_for_n_plots(labels, n_plots)\n",
        "    xlabels = prep_for_n_plots(xlabels, n_plots)\n",
        "    ylabels = prep_for_n_plots(ylabels, n_plots)\n",
        "    titles = prep_for_n_plots(titles, n_plots)\n",
        "    legend_loc = prep_for_n_plots(legend_loc, n_plots)\n",
        "\n",
        "    if not is_iterable(single_plot_sz):\n",
        "        single_plot_sz = [single_plot_sz, single_plot_sz]\n",
        "\n",
        "    figsize = [single_plot_sz[0] * n_plots, single_plot_sz[1]]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=n_plots, figsize=figsize)\n",
        "\n",
        "    if n_plots==1:\n",
        "      ax = [ax]\n",
        "\n",
        "    for axi, x, y, label, xlabel, ylabel, title, loc in zip(ax, xs, ys, labels, xlabels, ylabels, titles, legend_loc):\n",
        "        if y is None:\n",
        "            # placeholder for empty plot - to be filled by the caller\n",
        "            continue\n",
        "\n",
        "        if is_iterable(y[0]):\n",
        "            n = len(y)\n",
        "            x = prep_for_n_plots(x, n)\n",
        "            label = prep_for_n_plots(label, n)\n",
        "\n",
        "            for xi, yi, labeli in zip(x, y, label):\n",
        "                if xi is None:\n",
        "                  axi.plot(yi, label=labeli)\n",
        "                else:\n",
        "                  axi.plot(xi, yi, label=labeli)\n",
        "            axi.legend(loc=loc)\n",
        "        else:\n",
        "            if x is None:\n",
        "              axi.plot(y, label=label)\n",
        "            else:\n",
        "              axi.plot(x, y, label=label)\n",
        "        axi.set_xlabel(xlabel)\n",
        "        axi.set_ylabel(ylabel)\n",
        "        axi.set_title(title)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')  # use first available GPU\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sslFoqwK0haV"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz7LITEmO56u"
      },
      "source": [
        "# 1. Train Diffusion Model: butterflies dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iiSwzwQ1fZW"
      },
      "source": [
        "comlpete the code to make it work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq_m-E31_3g-"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFJDIkQX1DcH"
      },
      "outputs": [],
      "source": [
        "# inspect the `dataset`.\n",
        "# what are the elements of `dataset['images']` array?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBVh-52pHXaf"
      },
      "outputs": [],
      "source": [
        "raw_sz = # find max dimension of the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6j9S5rh_lds"
      },
      "outputs": [],
      "source": [
        "# it is convenient to store configurations as `dataclass`\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    raw_size = raw_sz\n",
        "    dataset_name = dataset_name\n",
        "    image_size = 32  # the generated image resolution  - keep it small for fast training\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16  # how many images to sample during evaluation\n",
        "    num_epochs = 150\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 10\n",
        "    save_model_epochs = 30\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = \"ddpm-butterflies-32\"\n",
        "\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 2023\n",
        "\n",
        "\n",
        "config = TrainingConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT-UvDOhAFqc"
      },
      "outputs": [],
      "source": [
        "def to_np_showable(pt_img):\n",
        "  np_im = # convert pt_img to numpy format\n",
        "  if len(np_im.shape) == 4:\n",
        "    np_im = np_im[0]\n",
        "\n",
        "  if np_im.shape[0] > 3:\n",
        "    np_im = np_im[-3:]\n",
        "\n",
        "  return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
        "\n",
        "def plot_im(im, is_torch=True):\n",
        "  plt.imshow(to_np_showable(im) if ...   # complete the line\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "def plot_im_samples(ds, col, n=5, is_torch=False):\n",
        "  fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
        "  for i, image in enumerate(ds[:n][col]):\n",
        "      axs[i].imshow(to_np_showable(image) if is_torch else image)\n",
        "      axs[i].set_axis_off()\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAxjomG9AitN"
      },
      "outputs": [],
      "source": [
        "# use `plot_im_samples` to visualize column 'image' in the `dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBM38PUPAQ-s"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(config.raw_size, pad_if_needed=True, fill=255.),\n",
        "        transforms.Resize # resize to the input image size given in the config file\n",
        "        ???,  # at random apply the horizontal flip to the samples\n",
        "        ???,  # convert to the torch tensor\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuThgdueAQCd"
      },
      "outputs": [],
      "source": [
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "\n",
        "    examples['images'] = images\n",
        "    return examples\n",
        "\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnXbVGtHANMe"
      },
      "outputs": [],
      "source": [
        "# visualize the dataset again, after applying the transformation.\n",
        "# What has to be changed?\n",
        "# Does what you see agree with what you expect to see?\n",
        "# What if you query the same sample multiple times?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzH1KPRf0i62"
      },
      "outputs": [],
      "source": [
        "def images_collate_fn(batch):\n",
        "    # collate only the 'images' column from each data sample in the batch\n",
        "    images = [item['images'] for item in batch]\n",
        "    return {'images': torch.stack(images)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH92hhbfAq1H"
      },
      "outputs": [],
      "source": [
        "train_dataloader = # create data loader with the batch size given in the config, and collatetion functions set to `images_collate_fn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHxemiJ5Cmla"
      },
      "outputs": [],
      "source": [
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(64, ???, 256),  # fill the missing number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16jASb8zxMuE"
      },
      "source": [
        "\n",
        "## Inspect U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyIIf8eC5GKi"
      },
      "source": [
        "Verify that U-Net output size matched the input:\n",
        "1. generate noisy sample\n",
        "2. pass through the model\n",
        "3. compare input and output sizes.\n",
        "\n",
        "What parameters is the model expecting as input?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPvatcZNxSHY"
      },
      "source": [
        "## Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_G-jjyV5fJO"
      },
      "outputs": [],
      "source": [
        "# take one sample\n",
        "sample_image = dataset[0][\"images\"].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wlpLcGw6uwE"
      },
      "source": [
        "inspect how samlple looks after 5, 20, 80, 320 diffusion steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4UhjFZAC5oj"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise = ....  # random noise tensor\n",
        "diffusion_step_idx = ... #\n",
        "timestep = torch.LongTensor([diffusion_step_idx])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timestep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trvIV_uFDCzF"
      },
      "outputs": [],
      "source": [
        "plot_im(noisy_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8NhxYPvEFYl"
      },
      "outputs": [],
      "source": [
        "# what does model predict?\n",
        ".... = model(noisy_image, timesteps).sample\n",
        "\n",
        "# what does the loss compare to what?\n",
        "loss = F.mse_loss("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt5K3OrXEc12"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=  # number of elements in data loder times numebr of epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8LUkEBEvHR"
      },
      "outputs": [],
      "source": [
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=config.eval_batch_size,\n",
        "        generator=torch.manual_seed(config.seed),\n",
        "        output_type = 'np.array'\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    images = pipeline.numpy_to_pil(images[..., -3:])\n",
        "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kaax2fbo9tQj"
      },
      "source": [
        "go carefully through the lines and understand what is going on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQyRPp6EExOP"
      },
      "outputs": [],
      "source": [
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything - allows for easy distributed training.\n",
        "    # merging results should happen under `if accelerator.is_main_process`\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch[\"images\"]\n",
        "            # Sample noise to add to the images\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n",
        "            ).long()\n",
        "\n",
        "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-AhejR8FVix"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "# oaarameters to the train loop\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yud9qUMbFaj3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
        "Image.open(sample_images[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fQzYe6XJ91v"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmiOulVIev0o"
      },
      "outputs": [],
      "source": [
        "#! ps | grep tensorboard\n",
        "# !kill xxxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoMUwTiFcrBK"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/ddpm-butterflies-32/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evrpx4ERe5U-"
      },
      "source": [
        "# 2. Train Latent Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVsXnnLa-l7a"
      },
      "source": [
        "Save and stop this notebook.\n",
        "\n",
        "Then :\n",
        "1. Train a Fully Convolutional (no flattening) autoencoder to create low-dimensional butterfly embedding ((3, 512, 512)  -> (32, 16, 16)). Save the trained model on your google drive\n",
        "2. Save dataset of latent representations (see the AE notebook)\n",
        "3. Adapt this filled notebook to a) 32 instead of 3 channels; b) saved dataset instead of the butterfly one.\n",
        "4. Train the LDM till convergence, and save on the google drive. How do you monitor convergence?\n",
        "5. Generate 16 samples.\n",
        "6. Pass them through the decoder model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrjGNPkOBbTJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AiWNh7VZz71D",
        "YgWXQ4B-0Vs2",
        "Rz7LITEmO56u",
        "evrpx4ERe5U-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}